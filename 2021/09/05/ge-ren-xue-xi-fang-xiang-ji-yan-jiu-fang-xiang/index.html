<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="个人学习方向及研究方向, 博客制作,个人经验分享,Unity,人工智能等">
    <meta name="description" content="本站记录本人各种学习的旅途，用于巩固自我并启发后来人">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>个人学习方向及研究方向 | 微笑紫瞳星</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>



   <style>
    body{
       background-image: url(https://cdn.jsdelivr.net/gh/Tokisaki-Galaxy/res/site/medias/background.jpg);
       background-repeat:no-repeat;
       background-size:cover;
       background-attachment:fixed;
    }
</style>



<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">微笑紫瞳星</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">微笑紫瞳星</div>
        <div class="logo-desc">
            
            本站记录本人各种学习的旅途，用于巩固自我并启发后来人
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/18.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">个人学习方向及研究方向</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95/">
                                <span class="chip bg-color">研究方法</span>
                            </a>
                        
                            <a href="/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/">
                                <span class="chip bg-color">论文写作</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" class="post-category">
                                学习方法
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-09-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2021-10-12
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    11.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    48 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        
        <!-- 代码块折行 -->
        <style type="text/css">
            code[class*="language-"], pre[class*="language-"] { white-space: pre-wrap !important; }
        </style>
        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p> 2021年9月5日更新：</p>
<h2 id="深度强化学习研究方向概述"><a href="#深度强化学习研究方向概述" class="headerlink" title="深度强化学习研究方向概述"></a>深度强化学习研究方向概述</h2><p>注：本段内容摘自知乎<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/342919579">https://zhuanlan.zhihu.com/p/342919579</a></p>
<p><strong>赶时间请直接看加粗的四种算法</strong>，它们占据不同的生态位，请根据实际任务需要去选择他们，在强化学习的子领域（多智能体、分层强化学习、逆向强化学习也会以它们为基础开发新的算法）：</p>
<ul>
<li>离散动作空间推荐：<strong>Dueling DoubleQN（D3QN）</strong></li>
<li>连续动作空间推荐：擅长调参就用<strong>TD3</strong>，不擅长调参就用<strong>PPO或SAC</strong>，如果训练环境 Reward function 都是初学者写的，那就用PPO</li>
</ul>
<p>来自另外一个大佬的建议：对于连续控制任务，推荐SAC、TD3和PPO，三种算法都值得试一试并从中择优；对于离散控制任务，推荐SAC-Discrete（即离散版SAC）和PPO。</p>
<h3 id="1-离散的动作空间-discrete-action-space"><a href="#1-离散的动作空间-discrete-action-space" class="headerlink" title="1.离散的动作空间 discrete action space"></a>1.离散的动作空间 discrete action space</h3><ul>
<li><p><strong>DQN（Deep Q Network）</strong>可用于入门深度强化学习，使用一个Q Network来估计Q值，从而替换了 Q-table，完成从离散状态空间到连续状态空间的跨越。Q Network 会对每一个离散动作的Q值进行估计，执行的时候选择Q值最高的动作（greedy 策略）。并使用 epslion-greedy 策略进行探索（探索的时候，有很小的概率随机执行动作），来获得各种动作的训练数据。</p>
</li>
<li><p><strong>DDQN（Double DQN）</strong>更加稳定，因为最优化操作会传播高估误差，所以她同时训练两个Q network并选择较小的Q值用于计算TD-error，降低高估误差。详见和DDQN一样采用了两个估值网络的TD3 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86297106">曾伊言：强化学习算法TD3论文的翻译与解读</a></p>
</li>
<li><p><strong><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1511.06581">Dueling DQN</a>**，Dueling DQN 使用了优势函数 advantage function（A3C也用了）：它只估计state的Q值，不考虑动作，好的策略能将state 导向一个更有优势的局面。原本DQN对一个state的Q值进行估计时，它需要等到</strong>为每个离散动作收集到数据后<strong>，才能进行准确估值。然而，在某些state下，采取不同的action并不会对Q值造成多大的影响，因此Dueling DQN 结合了 优势函数估计的Q值 与 原本DQN对不同动作估计的Q值。使得在某些state下，Dueling DQN 能在</strong>只收集到一个离散动作的数据后**，直接得到准确的估值。当某些环境中，存在大量不受action影响的state，此时Dueling DQN能学得比DQN更快。</p>
</li>
<li><p><strong>D3QN（Dueling Double DQN）。</strong>Dueling DQN 与Double DQN 相互兼容，一起用效果很好。简单，泛用，没有使用禁忌。任何一个刚入门的人都能独立地在前两种算法的基础上改出D3QN。在论文中使用了D3QN应该引用DuelingDQN 与 DoubleDQN的文章。</p>
</li>
<li><p><strong>Noisy DQN</strong>，探索能力稍强。Noisy DQN 把噪声添加到网络的输出层之前值。原本Q值较大的动作在添加噪声后Q值变大的概率也比较大。这种探索比epslion-greedy随机选一个动作去执行更好，至少这种针对性的探索既保证了探索动作多样，也提高了探索效率。</p>
</li>
</ul>
<p>====估计Q值的期望↑ ↓估计Q值的分布====</p>
<ul>
<li><strong>Distributional RL 值分布RL（C51，Distributional Perspective RL）</strong>。在DQN中，Q Network 拟合了Q值的期望，期望可以用一个数值去描述，比较简单。在值分布DQN中，Q Network 拟合了Q值的分布，Q值分布的描述就要麻烦一些了，但是训练效果更好。为C51的算法使用了这种方法，C表示Categorical，51表示他们将值分布划分51个grid。最终在雅达利游戏 Atari Game 上取得好结果。</li>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1710.10044.pdf">QR-DQN</a><strong>（分位数回归</strong> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40681570">Quantile Regression</a><strong>）</strong>，使用N个分位数去描述Q值分布（这种方法比C51划分51个grid的方法更妙，我推荐看 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/138091493">QR-DQN - Frank Tian</a>）。根据分位数的分布画出核分布曲线，详见 <a href="https://link.zhihu.com/?target=https://aakinshin.net/posts/qrde-hd/">Quantile-respectful density estimation based on the Harrell-Davis quantile estimator</a></li>
<li><strong>Rainbow DQN</strong>，上面提及的DQN变体很多是相互兼容的，因此 David Sliver 他们整合了这些变体，称为Rainbow。</li>
<li><strong>Ape-X DQN（Distributed Prioritized Experience Replay）</strong>，也是 David Sliver 他们做的。使用了Distributed training，用多个进程创建了多个actor去与环境交互，然后使用收集到的数据去训练同一个learner，用来加快训练速度。Prioritized Experience Replay（优先经验回放 PER 下面会讲）。Ape-X通过充分利用CPU资源，合理利用GPU，从而加快了训练速度。注意，这不等同于减少训练总步数。NVIDIA 有一个叫 Apex的库，用于加速计算。</li>
</ul>
<hr>
<h3 id="2-连续的动作空间-continuous-action-space"><a href="#2-连续的动作空间-continuous-action-space" class="headerlink" title="2.连续的动作空间 continuous action space"></a>2.连续的动作空间 continuous action space</h3><ul>
<li><strong>DDPG（Deep DPG ）</strong>，可用于入门连续动作空间的DRL算法。DPG 确定策略梯度算法，直接让策略网络输出action，成功在连续动作空间任务上训练出能用的策略，但是它使用 OU-noise 这种有很多超参数的方法去探索环境，训练慢，且不稳定。</li>
<li><strong>soft target update（软更新）</strong>，用来稳定训练的方法，非常好用，公式是 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%27+=+%5Ctau+%5Ctheta%27+++(1-%5Ctau)%5Ctheta" alt="[公式]"> ，其中 theta是使用梯度进行更新的网络参数，theta’ 是使用了软更新的目标网络target network参数，tau略小于1。软更新让参数的更新不至于发生剧变，从而稳定了训练。从DDPG开始就广泛使用，并且在深度学习的其他领域也能看到它的身影，如 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/159765213">谷歌自监督 BYOL Bootstrap Your Own Latent</a> ，看论文的公式（1），就用了soft target update</li>
<li><strong>TD3（TDDD，Twin Delay DDPG）</strong>，擅长调参的人才建议用，因为它影响训练的敏感超参数很多。它从Double DQN那里继承了Twin Critic，用来降低高估误差；它用来和随机策略梯度很像的方法：计算用于更新TD-error的Q值时，给action加上了噪声，用于让Critic拟合更平滑的Q值估计函数。TD3建议 延迟更新目标网络，即多更新几次网络后，再使用 soft update 将网络更新到target network上，我认为这没有多大用，后来的其他算法也不用这个技巧。TD3还建议在计算Q值时，为动作添加一个噪声，用于平滑Critic函数，在确定策略中，TD3这么用很像“随机策略”。详见 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86297106">曾伊言：强化学习算法TD3论文的翻译与解读</a></li>
<li><strong>D4PG（Distributed Distributional DDPG）</strong>，这篇文章做了实验，证明了一些大家都知道好用的trick是好用的。Distributed：它像 Ape-X一样用了 多线程开了actors 加快训练速度，Distributional：Q值分布RL（看前面的C51、QR-DQN）。DDPG探索能力差的特点，它也完好无缺地继承了。</li>
</ul>
<blockquote>
<p>在C51算法论文标题中，Distributional Perspective 指 Q值分布的表示。在Ape-X DQN 标题中， Distributed training 指分布式训练。我曾混淆过它们。</p>
</blockquote>
<p>====确定策略梯度↑ ↓ 随机策略梯度====</p>
<ul>
<li><strong>Stochastic Policy Gradient 随机策略梯度</strong>，随机策略的探索能力更好。随机策略网络会输出action的分布（通常输出高斯分布 均值 与 方差，少数任务下用其他分布），探索的噪声大小由智能体自己决定，更加灵活。但是这对算法提出了更高的要求。</li>
<li><strong>A3C（Asynchronous Advantage Actor-Critic）</strong>，Asynchronous 指开启多个actor 在环境中探索，并异步更新。原本DDPG的Critic 是 Q(s, a)，根据state-action pair 估计Q值，优势函数只使用 state 去估计Q值，<strong>这是很好的创新：降低了随机策略梯度算法估计Q值的难度</strong>。<strong>然而优势函数有明显缺陷</strong>：不是任何时刻 action 都会影响 state的转移（详见 Dueling DQN），因此这个算法只适合入门学习「优势函数 advantage function」。如果你看到新论文还在使用A3C，那么你要怀疑其作者RL的水平。此外，A3C算法有离散动作版本，也有连续动作版本。A2C 指的是没有Asynchronous 的版本。</li>
<li><strong>TRPO（Trust Region Policy Optimization）</strong>，信任域 Trust Region。连续动作空间无法每一个动作都搜索一遍，因此大部分情况下只能靠猜。如果要猜，就只能在信任域内部去猜。TRPO将每一次对策略的更新都限制了信任域内，从而极大地增强了训练的稳定性。可惜信任域的计算量太大了，因此其作者推出了PPO，如果你PPO论文看不懂，那么建议你先看TRPO。如果你看到新论文还在使用TRPO，那么你要怀疑其作者RL的水平。</li>
<li><strong>PPO（Proximal PO 近端策略搜索）</strong>，训练稳定，调参简单，robust（稳健、耐操）。PPO对TRPO的信任域计算过程进行简化，论文中用的词是 surrogate objective。PPO动作的噪声方差是一个可训练的矢量（与动作矢量相同形状），而不由网络输出，这样做增强了PPO的稳健性 robustness。</li>
<li><strong>PPO+GAE（</strong><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1506.02438">Generalized Advantage Estimation</a><strong>）</strong>，训练最稳定，调参最简单，适合高维状态 High-dimensional state，<strong>但是环境不能有太多随机因数</strong>。GAE会根据经验轨迹 trajectory 生成优势函数估计值，然后让Critic去拟合这个值。在这样的调整下，在随机因素小的环境中，不需要太多 trajectory 即可描述当前的策略。尽管GAE可以用于多种RL算法，但是她与PPO这种On-policy 的相性最好。</li>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2009.04416">PPG</a><strong>（Proximal Policy Gradient）</strong>，A3C、PPO 都是同策略 On-policy，它要求：在环境中探索并产生训练数据的策略 与 被更新的策略网络 一定得是同一个策略。她们需要删掉已旧策略的数据，然后使用新策略在环境中重新收集。为了让PPO也能用 off-policy 的数据来训练，PPG诞生了，思路挺简单的，原本的On-policy PPO部分该干啥干啥，额外引入一个使用off-policy数据进行训练的Critic，让它与PPO的Critic共享参数，也就是Auxiliary Task，参见 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/342150033">Flood Sung：深度解读：Policy Gradient，PPO及PPG</a> ，以及<a href="https://link.zhihu.com/?target=https://zhuanlan.zhi%3Cb%3Ehu.com/p/14851%3C/b%3E6171">白辰甲：强化学习中自适应的辅助任务加权(Adaptive Auxiliary Task Weighting)</a>。这种算法并不是在任何情况下都能比PPO好，因为PPG涉及到Auxiliary task，这要求她尽可能收集更多的训练数据，并在大batch size 下面才能表现得更好。</li>
<li>Interpolated Policy Gradient NIPS.2017，<strong>反面例子</strong>，它试图基于 On-policy TRPO 改出一个 能利用 off-policy 数据的TRPO来（就像PPO→PPG），然而Interpolated Policy Gradient 强行地、错误地使用off-policy 数据对Critic 进行训练。结果在其论文中放出的结果中，它的性能甚至比A3C还差，只是比TRPO、DDPG略好（但是它故意没有和比它好的算法在同一个任务下比较：论文结果很诚实，但是用事实说谎）。</li>
<li>Soft Q-learning（Deep Energy Based Policy）是SAC的前身，最大熵算法的萌芽，她的作者后来写出了SAC（都叫soft ***），你可以跳过Soft QL，直接看SAC的论文。<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/76681229">黄伟：Soft Q-Learning论文阅读笔记</a></li>
<li><strong>SAC（Soft Actor-Critic with maximum entropy 最大熵）</strong>，训练很快，探索能力好，但是很依赖Reward Function，不像PPO那样随便整一个Reward function 也能训练。PPO算法会计算新旧策略的差异（计算两个分布之间的距离），并让这个差异保持在信任域内，且不至于太小。SAC算法不是on-policy算法，不容易计算新旧策略的差异，所以它在优化时最大化策略的熵（动作的方差越大，策略的熵越高）。</li>
<li><strong>SAC（Automating Entropy Adjustment/ Automating Temperature Parameter</strong> <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> <strong>自动调整温度系数并维持策略的熵在某个值附近）</strong>一般我们使用的SAC是这个版本的SAC，它能自动调整一个叫温度系数alpha 的超参数（温度越高，熵越大）。SAC的策略网络的优化目标=累计收益+ alpha*策略的熵。一般在训练后期，策略找到合适的action分布均值时，它的action分布方差越小，其收益越高，因而对“累计收益”进行优化，会让策略熵倾向于减小。SAC会自动选择合适的温度系数，让策略的熵保持一种适合训练的动态平衡。SAC会事先确定一个目标熵 target entropy（论文作者的推荐值是 log(action_dim)），如果策略熵大于此值，则将alpha调小，反之亦然。从这个角度看，SAC就不是最大化策略熵了，而是将策略熵限制在某个合适大小内，这点又与PPO的“保持在信任域内，且不至于太小”不谋而合</li>
</ul>
<hr>
<h3 id="3-混合的动作空间-hybrid-action-space"><a href="#3-混合的动作空间-hybrid-action-space" class="headerlink" title="3.混合的动作空间 hybrid action space"></a>3.混合的动作空间 hybrid action space</h3><p>在实际任务中，混合动作的需求经常出现：如王者荣耀游戏既需要离散动作（选择技能），又需要连续动作（移动角色）。只要入门了强化学习，就很容易独立地想出以下这些方法，所以我没有把它们放在前面：</p>
<ul>
<li><strong>强行使用DQN类算法，把连续动作分成多个离散动作：</strong>不建议这么做，这破坏了连续动作的优势。一个良性的神经网络会是一个平滑的函数（k-Lipschitz 连续），相近的输入会有相似的输出。在连续的动作空间开区间[-1, +1]中，智能体会在学了-1，+1两个样本后，猜测0的样本可能介于 -1，+1 之间。而强行将拆分为离散动作 -1，0，+1之后（无论拆分多么精细），它都猜不出 0的样本，一定要收集到 0的样本才能学习。此外，精细的拆分会增加离散动作个数，尽管更加逼近连续动作，但会增加训练成本。</li>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1910.07207">SAC for Discrete Action Space</a><strong>，把输出的连续动作当成是离散动作的执行概率：</strong>SAC for Discrete Action Space 这个算法提供了将连续动作算法SAC应用在离散动作的一条技术路线：把这个输出的动作矢量当成每个动作的执行概率。一般可以直接把离散动作部分全部改成连续动作，然后套用连续动作算法，这方法简单，但是不一定最好的。</li>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1810.06394">P-DQN</a> <strong>（Parameterized DQN），把DQN和DDPG合起来</strong>：Q network 会输出每个动作对应的Q值，执行的时候选择Q值高的动作。DDPG与其他策略梯度算法，让Critic预测 state-action的Q值，然后用Critic 提供的梯度去优化Actor，让Actor输出Q值高的动作。现在，对于一个混合动作来说，我们可以让Critic学习Q Network，让Critic也为每个离散动作输出对应的Q值，然后用Critic中 arg max Qi 提供梯度优化Actor。这是很容易独立想出来的方法，相比前两个方案缺陷更小。</li>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1903.01344">H-PPO</a> <strong>（Hybrid PPO），同时让策略网络输出混合动作</strong>。连续动作（策略梯度）算法中：DDPG、TD3、SAC使用 状态-动作值函数 Q(state, action)，A3C、<strong>PPO使用 状态值函数 Q(state)**。离散动作无法像连续动作一样将一个action输入到 Q(state, action) 里，因此 Hybird PPO选择了PPO。于是它的策略网络会像Q Network 一样为离散动作输出不同的Q值，也像PPO 一样输出连续动作。（</strong>警告，2021-03 前它依然没有开源代码，但论文描述的方法无误<strong>）。还有 <a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2001.00449">H-MPO</a>（</strong>Hybrid MPO**），MPO是PPO算法的改进版。</li>
</ul>
<p>详见<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1903.01344">Hybird-PPO 2019-03</a> 这篇文章的 Related Work。知乎上也有 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104581527">黑猫紧张：PN-46: H-PPO for Hybrid Action Space (IJCAI 2019)</a></p>
<h3 id="4-改进经验回放，以适应稀疏奖励-sparse-reward"><a href="#4-改进经验回放，以适应稀疏奖励-sparse-reward" class="headerlink" title="4.改进经验回放，以适应稀疏奖励 sparse reward"></a>4.改进经验回放，以适应稀疏奖励 sparse reward</h3><p>训练LunarLander安全降落，它的奖励reward 在降落后+200，坠毁-100。当它还在空中时做任何动作都不会得到绝对值这么大的奖励。这样的奖励是稀疏的。一些算法（（其实它的奖励函数会根据飞行器在空中的稳定程度、燃料消耗给出一个很小的reward，在这种）</p>
<ul>
<li><strong>Prioritized sweeping 优先清理：</strong>根据紧要程度调整样本的更新顺序，优先使用某些样本进行更新，用于加速训练，PER就是沿着这种思想发展出来的</li>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1511.05952.pdf">PER</a><strong>（优先经验回放 Prioritized Experience Replay）</strong>使用不同顺序的样本进行对网络进行训练，并将不同顺序对应的Q值差异保存下来，以此为依据调整样本更新顺序，用于加速训练。</li>
<li><a href="https://link.zhihu.com/?target=https://papers.nips.cc/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf">HER</a><strong>（后见经验回放 Hindsight Experience Replay）</strong>构建可以把失败经验也利用起来的经验池，提高稀疏奖励下对各种失败探索经验的利用效率。</li>
</ul>
<p>这种操作需要消耗CPU算力去完成。在奖励不稀疏的环境下，用了不会明显提升。在一些环境中，上面这类算法必不可少。例如 <a href="https://link.zhihu.com/?target=https://gym.openai.com/envs/%23robotics">Gym 基于收费MuJoCo 的机械臂环境 Robotics Fetch***</a> ，以及 基于<a href="https://link.zhihu.com/?target=https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/kuka.py">开源PyBullet的机械臂 KukaBulletEnv-v0</a> 。如果不用这类算法，那么我们需要花费更多精力去设计Reward function。</p>
<h3 id="5-在RL中使用RNN"><a href="#5-在RL中使用RNN" class="headerlink" title="5.在RL中使用RNN"></a>5.在RL中使用RNN</h3><p>有时候，我们需要观察连续的几个状态才能获得完整的状态。例如赛车游戏，打砖块游戏，我们在只观测单独一帧图片时（部分可观测 Partially Observable state），无法知晓物体的运动速度。因此将相邻几个可观测的状态堆叠起来（stack）可以很好地应对这些难题。</p>
<p>然而，堆叠相邻状态无法应对所有的 PO-MDPs。于是大家想到了用RNN（<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/94757947">RNN的入门例子：根据前9年的数据预测后3年的客流</a>）。然而RNN需要使用一整段序列去训练，很不适合TD-errors（贝尔曼公式）的更新方式。如图：普通Actor网络的输入只有state，而使用RNN的Actor网络的输入其实是 Partially Observable state 以及 hidden state。这意味着输入critic网络进行Q值评估的state不完整（不包含RNN内部的hidden state）。为了解决<strong>环境的PO-MDPs难题</strong>，我们直接引入RNN结构。然而引入RNN结构的行为又给RL带来了<strong>RNN内部的PO-MDPs难题</strong>。</p>
<p><img src="https://pic4.zhimg.com/80/v2-bfb5ca44f103cf1de24aaff13c74c527_720w.jpg" alt="img"></p>
<p>尽管在on-policy算法中，如果使用完整的轨迹（trajectory）进行更新，那么可以缓解critic观测不到 hidden state给训练到来的影响。（如腾讯绝悟的PPO算法使用了RNN），但是这个问题还是没有很好地得到解决。如果想要了解更多使用了RNN的RL算法，请看 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/349041830">羽根：【强化学习TOOLBOX 3】RNN, DRQN, R2D2</a> 。2021年前，我极少复现过效果好的RL+RNN算法，因此我不推荐任何使用了RL+RNN的算法。</p>
<p>虽说 RL＋RNN 有各种问题，但是 R2D2 NGU.2020 Agent57.2020 这些算法都用了RNN。<strong>我并非认为RNN＋RL 不可行，而是认为 在RL中训练RNN 有太多 trick，复现困难。</strong>其中，Agent57 训练RNN 的方法很值得借鉴，在论文E. implementation details 写得比较具体。如: 在env transition，也保存了recurrent state)。我有空会继续补充以下三种算法，写于2021-6-18：</p>
<p><a href="https://link.zhihu.com/?target=https://openreview.net/pdf?id=r1lyTjAqYX">R2D2</a>（<strong>Recurrent Replay Distributed DQN.</strong> ICLR.2019）</p>
<p><a href="https://link.zhihu.com/?target=https://openreview.net/pdf?id=Sye57xStvB">NGU</a>（<strong>Never Give Up</strong>: Learning Directed Exploration Strategies. ICLR. 2020）</p>
<p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2003.13350">Agent57</a>（<strong>Agent57</strong>: Outperforming the Atari Human Benchmark. 2020）</p>
<p>一种在RL里使用RNN的方法<strong>（Note RL+RNN）：让使用了RNN 的Actor 输出hidden state 作为action，就像人类记笔记一样。</strong>把 part state + hidden state 视为完整的state，把 hidden state + action 视为完整的 action。无论是 state-value function 还是 state-action value function 都能使用这个方法。</p>
<p><img src="https://pic4.zhimg.com/80/v2-6902e22cfa923c026e01b19f9612e287_720w.jpg" alt="img"></p>
<h3 id="6-强化学习探索"><a href="#6-强化学习探索" class="headerlink" title="6.强化学习探索"></a>6.强化学习探索</h3><p>收集不同的state、在同一个state下尝试不同的action 的探索过程非常重要。通过探索收集到足够多的数据，是我们用RL训练出接近最优策略的前提。下面这篇系统介绍了各种探索策略：</p>
<p><a href="https://link.zhihu.com/?target=https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html">Exploration Strategies in Deep Reinforcement Learning - LiLianWeng</a> （我也推荐看她的其他文章）</p>
<blockquote>
<p>最近的 First return, then explore Nature.2021 是Go-explore. 2018的升级版</p>
</blockquote>
<h3 id="7-多智能体算法-MultiAgent-RL"><a href="#7-多智能体算法-MultiAgent-RL" class="headerlink" title="7.多智能体算法 MultiAgent RL"></a>7.多智能体算法 MultiAgent RL</h3><p>多智能体算法的综述：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2011.00583">An Overview of Multi-agent Reinforcement Learning from Game Theoretical Perspective</a> - 2020-12。在比对了多份MARL综述后，只推荐这一篇，它的目录与注释如下：</p>
<pre class=" language-python3"><code class="language-python3">Contents
1 Introduction
    ...
    1.3 2019: A Booming Year for MARL  # 写在2019年前的MARL综述可看性低
2 Single-Agent Reinforcement Learning 
    ...
3 Multi-Agent Reinforcement Learning
    ...
    3.2.5 Partially Observable Settings  # state部分可观测的MDPs
    3.3 Problem Formulation: Extensive-Form Game
    3.3.1 Normal-Form Representation  # 普通形式
    3.3.2 Sequence-Form Representation  # 序列形式
    3.4 Solving Extensive-form Games 
    3.4.1 Solutions to Perfect-Information Games  # 完全信息博弈
    3.4.2 Solutions to Imperfect-Information Games  # 非完全信息博弈（有战场迷雾）
4 The Grand Challenges 38
    4.1 The Combinatorial Complexity  # 动作空间变大，搜索策略变难
    4.2 The Multi-Dimensional Learning Objectives  # 状态空间变大，搜索策略变难
    4.3 The Non-Stationarity Issue  # MARL中，每个智能体的策略总发生改变，导致外部环境不稳定
    4.4 The Scalability Issue when N >> 2  # 智能体数量增大，甚至数量改变
5 A Survey of MARL Surveys  # 推荐有单智能体基础的人得先看 MARL算法的分类
    ...
6 Learning in Identical-Interest Games  # 合作的MARL
    6.1 Stochastic Team Games 
    6.1.1 Solutions via Q-function Factorisation  # 基于Q值
    6.1.2 Solutions via Multi-Agent Soft Learning   # 基于随机策略
    6.2 Dec-POMDP  #  decentralized PO-MDPs 每个智能体智能看到局部的state 导致的部分可观测
    6.3 Networked Multi-Agent MDP  智能体是异构的heterogeneous，而非同源homogeneous
    6.4 Stochastic Potential Games
7 Learning in Zero-Sum Games  # 竞争的MARL （包含了团体间竞争，团体内合作的情况）
    ...
    7.3.1 Variations of Fictitious Play  # 虚拟博弈， 类似于 Model-based 做 Planning
    7.3.2 Counterfactual Regret Minimisation  # 反事实推理（向左错了，于是考虑向右是否更好）
    7.4 Policy Space Response Oracle  # 策略空间太大时，考虑用元博弈(meta-game)
    7.5 Online Markov Decision Process
    7.6 Turn-Based Stochastic Games  # 智能体之间轮流做决策，而不是同时做
8 Learning in General-Sum Games
    ...  # 混合了团队博弈合作team games 与 零和博弈竞争zero-sum games 的 General-Sum game
9 Learning in Games with N → +∞  # 无终止状态的博弈（需要考虑信任与背叛）
    9.1 Non-Cooperative Setting: Mean-Field Games  # 博弈平均场，把其他智能体也视为外部环境
    ...
END</code></pre>
<p>部分多智能体算法的代码以及少量介绍：<a href="https://link.zhihu.com/?target=https://github.com/starry-sky6688/StarCraft">starry-sky6688/StarCraft 在星际争霸环境 复现了多种多智能体强化学习算法</a></p>
<p>若智能体间通信没有受到限制（不限量，无延迟），那么我们完全可以把多智能体当成单智能体来处理。适用于部分可观测的MDPs的算法（<a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">Partially observable MDPs</a>），在多智能体任务中，每个视角有限的智能体观察到的只是 partially observable state。很多多智能体算法会参与 PO-MDPs 的讨论，由于每个智能体只能观察到局部信息而导致的部分可观测被称为 Dec-POMDP，在上面的MARL综述也有讨论。</p>
<h4 id="7-1-Tutorial-and-Books"><a href="#7-1-Tutorial-and-Books" class="headerlink" title="7.1 Tutorial and Books"></a>7.1 Tutorial and Books</h4><ul>
<li><a target="_blank" rel="noopener" href="https://ora.ox.ac.uk/objects/uuid:a55621b3-53c0-4e1b-ad1c-92438b57ffa4">Deep Multi-Agent Reinforcement Learning</a> by Jakob N Foerster, 2018. PhD Thesis.</li>
<li><a target="_blank" rel="noopener" href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118884614">Multi-Agent Machine Learning: A Reinforcement Approach</a> by H. M. Schwartz, 2014.</li>
<li><a target="_blank" rel="noopener" href="http://www.ecmlpkdd2013.org/wp-content/uploads/2013/09/Multiagent-Reinforcement-Learning.pdf">Multiagent Reinforcement Learning</a> by Daan Bloembergen, Daniel Hennes, Michael Kaisers, Peter Vrancx. ECML, 2013.</li>
<li><a target="_blank" rel="noopener" href="http://www.masfoundations.org/download.html">Multiagent systems: Algorithmic, game-theoretic, and logical foundations</a> by Shoham Y, Leyton-Brown K. Cambridge University Press, 2008.</li>
</ul>
<h4 id="7-2-Review-Papers"><a href="#7-2-Review-Papers" class="headerlink" title="7.2 Review Papers"></a>7.2 Review Papers</h4><ul>
<li><a target="_blank" rel="noopener" href="https://www.jair.org/index.php/jair/article/view/11396">A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems</a> by Silva, Felipe Leno da; Costa, Anna Helena Reali. JAIR, 2019.</li>
<li><a target="_blank" rel="noopener" href="https://www.ijcai.org/proceedings/2018/774">Autonomously Reusing Knowledge in Multiagent Reinforcement Learning</a> by Silva, Felipe Leno da; Taylor, Matthew E.; Costa, Anna Helena Reali. IJCAI, 2018.</li>
<li><a target="_blank" rel="noopener" href="https://project-archive.inf.ed.ac.uk/msc/20162091/msc_proj.pdf">Deep Reinforcement Learning Variants of Multi-Agent Learning Algorithms</a> by Castaneda A O. 2016.</li>
<li><a target="_blank" rel="noopener" href="https://www.jair.org/index.php/jair/article/view/10952">Evolutionary Dynamics of Multi-Agent Learning: A Survey</a> by Bloembergen, Daan, et al. JAIR, 2015.</li>
<li><a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/269100101_Game_Theory_and_Multi-agent_Reinforcement_Learning">Game theory and multi-agent reinforcement learning</a> by Nowé A, Vrancx P, De Hauwere Y M. Reinforcement Learning. Springer Berlin Heidelberg, 2012.</li>
<li><a target="_blank" rel="noopener" href="http://www.dcsc.tudelft.nl/~bdeschutter/pub/rep/10_003.pdf">Multi-agent reinforcement learning: An overview</a> by Buşoniu L, Babuška R, De Schutter B. Innovations in multi-agent systems and applications-1. Springer Berlin Heidelberg, 2010</li>
<li><a target="_blank" rel="noopener" href="http://www.dcsc.tudelft.nl/~bdeschutter/pub/rep/07_019.pdf">A comprehensive survey of multi-agent reinforcement learning</a> by Busoniu L, Babuska R, De Schutter B. IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews, 2008</li>
<li>[If multi-agent learning is the answer, what is the question?](<a target="_blank" rel="noopener" href="http://robotics.stanford.edu/~shoham/www">http://robotics.stanford.edu/~shoham/www</a> papers/LearningInMAS.pdf) by Shoham Y, Powers R, Grenager T. Artificial Intelligence, 2007.</li>
<li><a target="_blank" rel="noopener" href="http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/learningNeto05.pdf">From single-agent to multi-agent reinforcement learning: Foundational concepts and methods</a> by Neto G. Learning theory course, 2005.</li>
<li><a target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/bb9f/bee22eae2b47bbf304804a6ac07def1aecdb.pdf">Evolutionary game theory and multi-agent reinforcement learning</a> by Tuyls K, Nowé A. The Knowledge Engineering Review, 2005.</li>
<li><a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/221622801_An_Overview_of_Cooperative_and_Competitive_Multiagent_Learning">An Overview of Cooperative and Competitive Multiagent Learning</a> by Pieter Jan ’t HoenKarl TuylsLiviu PanaitSean LukeJ. A. La Poutré. AAMAS’s workshop LAMAS, 2005.</li>
<li><a target="_blank" rel="noopener" href="https://cs.gmu.edu/~eclab/papers/panait05cooperative.pdf">Cooperative multi-agent learning: the state of the art</a> by Liviu Panait and Sean Luke, 2005.</li>
</ul>
<h4 id="7-3-Framework-papers"><a href="#7-3-Framework-papers" class="headerlink" title="7.3 Framework papers"></a>7.3 Framework papers</h4><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.05438.pdf">Mean Field Multi-Agent Reinforcement Learning</a> by Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. ICML 2018.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.02275.pdf">Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments</a> by Lowe R, Wu Y, Tamar A, et al. arXiv, 2017.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.06182.pdf">Deep Decentralized Multi-task Multi-Agent RL under Partial Observability</a> by Omidshafiei S, Pazis J, Amato C, et al. arXiv, 2017.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.10069.pdf">Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games</a> by Peng P, Yuan Q, Wen Y, et al. arXiv, 2017.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.02702.pdf">Robust Adversarial Reinforcement Learning</a> by Lerrel Pinto, James Davidson, Rahul Sukthankar, Abhinav Gupta. arXiv, 2017.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1702.08887.pdf">Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning</a> by Foerster J, Nardelli N, Farquhar G, et al. arXiv, 2017.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1508.05328.pdf">Multiagent reinforcement learning with sparse interactions by negotiation and knowledge transfer</a> by Zhou L, Yang P, Chen C, et al. IEEE transactions on cybernetics, 2016.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1409.4561.pdf">Decentralised multi-agent reinforcement learning for dynamic and uncertain environments</a> by Marinescu A, Dusparic I, Taylor A, et al. arXiv, 2014.</li>
<li><a target="_blank" rel="noopener" href="http://irll.eecs.wsu.edu/wp-content/papercite-data/pdf/2014iat-holmesparker.pdf">CLEANing the reward: counterfactual actions to remove exploratory action noise in multiagent learning</a> by HolmesParker C, Taylor M E, Agogino A, et al. AAMAS, 2014.</li>
<li><a target="_blank" rel="noopener" href="http://www.fransoliehoek.net/docs/Amato13MSDM.pdf">Bayesian reinforcement learning for multiagent systems with state uncertainty</a> by Amato C, Oliehoek F A. MSDM Workshop, 2013.</li>
<li><a target="_blank" rel="noopener" href="http://www.weiss-gerhard.info/publications/AI_MAGAZINE_2012_TuylsWeiss.pdf">Multiagent learning: Basics, challenges, and prospects</a> by Tuyls, Karl, and Gerhard Weiss. AI Magazine, 2012.</li>
<li><a target="_blank" rel="noopener" href="http://icml2010.haifa.il.ibm.com/papers/191.pdf">Classes of multiagent q-learning dynamics with epsilon-greedy exploration</a> by Wunder M, Littman M L, Babes M. ICML, 2010.</li>
<li><a target="_blank" rel="noopener" href="http://www.machinelearning.org/proceedings/icml2007/papers/89.pdf">Conditional random fields for multi-agent reinforcement learning</a> by Zhang X, Aberdeen D, Vishwanathan S V N. ICML, 2007.</li>
<li><a target="_blank" rel="noopener" href="http://ama.imag.fr/~partalas/partalasmarl.pdf">Multi-agent reinforcement learning using strategies and voting</a> by Partalas, Ioannis, Ioannis Feneris, and Ioannis Vlahavas. ICTAI, 2007.</li>
<li><a target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/57fb/ae00e17c0d798559ebab0e8f4267e032f41d.pdf">A reinforcement learning scheme for a partially-observable multi-agent game</a> by Ishii S, Fujita H, Mitsutake M, et al. Machine Learning, 2005.</li>
<li><a target="_blank" rel="noopener" href="http://lib.tkk.fi/Diss/2004/isbn9512273594/article1.pdf">Asymmetric multiagent reinforcement learning</a> by Könönen V. Web Intelligence and Agent Systems, 2004.</li>
<li><a target="_blank" rel="noopener" href="http://dl.acm.org/citation.cfm?id=860686">Adaptive policy gradient in multiagent learning</a> by Banerjee B, Peng J. AAMAS, 2003.</li>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2171-reinforcement-learning-to-play-an-optimal-nash-equilibrium-in-team-markov-games.pdf">Reinforcement learning to play an optimal Nash equilibrium in team Markov games</a> by Wang X, Sandholm T. NIPS, 2002.</li>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S0004370202001212">Multiagent learning using a variable learning rate</a> by Michael Bowling and Manuela Veloso, 2002.</li>
<li><a target="_blank" rel="noopener" href="http://www.sts.rpi.edu/~rsun/si-mal/article3.pdf">Value-function reinforcement learning in Markov game</a> by Littman M L. Cognitive Systems Research, 2001.</li>
<li><a target="_blank" rel="noopener" href="http://researchers.lille.inria.fr/~ghavamza/my_website/Publications_files/agents01.pdf">Hierarchical multi-agent reinforcement learning</a> by Makar, Rajbala, Sridhar Mahadevan, and Mohammad Ghavamzadeh. The fifth international conference on Autonomous agents, 2001.</li>
<li><a target="_blank" rel="noopener" href="https://www.cs.cmu.edu/~mmv/papers/00TR-mike.pdf">An analysis of stochastic game theory for multiagent reinforcement learning</a> by Michael Bowling and Manuela Veloso, 2000.</li>
</ul>
<h4 id="7-4-Joint-action-learning"><a href="#7-4-Joint-action-learning" class="headerlink" title="7.4 Joint action learning"></a>7.4 Joint action learning</h4><ul>
<li><a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/~conitzer/awesomeML06.pdf">AWESOME: A general multiagent learning algorithm that converges in self-play and learns a best response against stationary opponents</a> by Conitzer V, Sandholm T. Machine Learning, 2007.</li>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2503-extending-q-learning-to-general-adaptive-multi-agent-systems.pdf">Extending Q-Learning to General Adaptive Multi-Agent Systems</a> by Tesauro, Gerald. NIPS, 2003.</li>
<li><a target="_blank" rel="noopener" href="http://www.lirmm.fr/~jq/Cours/3cycle/module/HuWellman98icml.pdf">Multiagent reinforcement learning: theoretical framework and an algorithm.</a> by Hu, Junling, and Michael P. Wellman. ICML, 1998.</li>
<li><a target="_blank" rel="noopener" href="http://www.aaai.org/Papers/AAAI/1998/AAAI98-106.pdf">The dynamics of reinforcement learning in cooperative multiagent systems</a> by Claus C, Boutilier C. AAAI, 1998.</li>
<li><a target="_blank" rel="noopener" href="https://www.cs.duke.edu/courses/spring07/cps296.3/littman94markov.pdf">Markov games as a framework for multi-agent reinforcement learning</a> by Littman, Michael L. ICML, 1994.</li>
</ul>
<h4 id="7-5-Cooperation-and-competition"><a href="#7-5-Cooperation-and-competition" class="headerlink" title="7.5 Cooperation and competition"></a>7.5 Cooperation and competition</h4><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.03748.pdf">Emergent complexity through multi-agent competition</a> by Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, Igor Mordatch, 2018.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1709.04326.pdf">Learning with opponent learning awareness</a> by Jakob Foerster, Richard Y. Chen2, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, Igor Mordatch, 2018.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1702.03037.pdf">Multi-agent Reinforcement Learning in Sequential Social Dilemmas</a> by Leibo J Z, Zambaldi V, Lanctot M, et al. arXiv, 2017. [<a target="_blank" rel="noopener" href="https://deepmind.com/blog/understanding-agent-cooperation/">Post</a>]</li>
<li><a target="_blank" rel="noopener" href="http://orca.st.usm.edu/~banerjee/papers/p530-ceren.pdf">Reinforcement Learning in Partially Observable Multiagent Settings: Monte Carlo Exploring Policies with PAC Bounds</a> by Roi Ceren, Prashant Doshi, and Bikramjit Banerjee, pp. 530-538, AAMAS 2016.</li>
<li><a target="_blank" rel="noopener" href="http://www.umiacs.umd.edu/~hal/docs/daume16opponent.pdf">Opponent Modeling in Deep Reinforcement Learning</a> by He H, Boyd-Graber J, Kwok K, et al. ICML, 2016.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.08779.pdf">Multiagent cooperation and competition with deep reinforcement learning</a> by Tampuu A, Matiisen T, Kodelja D, et al. arXiv, 2015.</li>
<li><a target="_blank" rel="noopener" href="http://www.uow.edu.au/~fren/documents/EMR_2013.pdf">Emotional multiagent reinforcement learning in social dilemmas</a> by Yu C, Zhang M, Ren F. International Conference on Principles and Practice of Multi-Agent Systems, 2013.</li>
<li><a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume9/bab08a/bab08a.pdf">Multi-agent reinforcement learning in common interest and fixed sum stochastic games: An experimental study</a> by Bab, Avraham, and Ronen I. Brafman. Journal of Machine Learning Research, 2008.</li>
<li><a target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/5120/d9f2c738ad223e9f8f14cb3fd5612239a35c.pdf">Combining policy search with planning in multi-agent cooperation</a> by Ma J, Cameron S. Robot Soccer World Cup, 2008.</li>
<li><a target="_blank" rel="noopener" href="http://www.jmlr.org/papers/volume7/kok06a/kok06a.pdf">Collaborative multiagent reinforcement learning by payoff propagation</a> by Kok J R, Vlassis N. JMLR, 2006.</li>
<li><a target="_blank" rel="noopener" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.107.335&amp;rep=rep1&amp;type=pdf">Learning to cooperate in multi-agent social dilemmas</a> by de Cote E M, Lazaric A, Restelli M. AAMAS, 2006.</li>
<li><a target="_blank" rel="noopener" href="http://www.machinelearning.org/proceedings/icml2005/papers/021_Learning_CrandallGoodrich.pdf">Learning to compete, compromise, and cooperate in repeated general-sum games</a> by Crandall J W, Goodrich M A. ICML, 2005.</li>
<li><a target="_blank" rel="noopener" href="http://www.machinelearning.org/proceedings/icml2004/papers/267.pdf">Sparse cooperative Q-learning</a> by Kok J R, Vlassis N. ICML, 2004.</li>
</ul>
<h4 id="7-6-Coordination"><a href="#7-6-Coordination" class="headerlink" title="7.6 Coordination"></a>7.6 Coordination</h4><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.03121.pdf">Coordinated Multi-Agent Imitation Learning</a> by Le H M, Yue Y, Carr P. arXiv, 2017.</li>
<li><a target="_blank" rel="noopener" href="http://mipc.inf.ed.ac.uk/2014/papers/mipc2014_hao_etal.pdf">Reinforcement social learning of coordination in networked cooperative multiagent systems</a> by Hao J, Huang D, Cai Y, et al. AAAI Workshop, 2014.</li>
<li><a target="_blank" rel="noopener" href="http://www.aamas-conference.org/Proceedings/aamas2013/docs/p1101.pdf">Coordinating multi-agent reinforcement learning with limited communication</a> by Zhang, Chongjie, and Victor Lesser. AAMAS, 2013.</li>
<li><a target="_blank" rel="noopener" href="http://www.ifaamas.org/Proceedings/aamas2012/papers/1B_1.pdf">Coordination guided reinforcement learning</a> by Lau Q P, Lee M L, Hsu W. AAMAS, 2012.</li>
<li><a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~cebly/Papers/bayesMARL.pdf">Coordination in multiagent reinforcement learning: a Bayesian approach</a> by Chalkiadakis G, Boutilier C. AAMAS, 2003.</li>
<li><a target="_blank" rel="noopener" href="https://users.cs.duke.edu/~parr/icml02.pdf">Coordinated reinforcement learning</a> by Guestrin C, Lagoudakis M, Parr R. ICML, 2002.</li>
<li><a target="_blank" rel="noopener" href="http://www.aaai.org/Papers/AAAI/2002/AAAI02-050.pdf">Reinforcement learning of coordination in cooperative multi-agent systems</a> by Kapetanakis S, Kudenko D. AAAI/IAAI, 2002.</li>
</ul>
<h4 id="7-7-Security"><a href="#7-7-Security" class="headerlink" title="7.7 Security"></a>7.7 Security</h4><ul>
<li><a target="_blank" rel="noopener" href="http://www.fransoliehoek.net/docs/Klima16LICMAS.pdf">Markov Security Games: Learning in Spatial Security Problems</a> by Klima R, Tuyls K, Oliehoek F. The Learning, Inference and Control of Multi-Agent Systems at NIPS, 2016.</li>
<li><a target="_blank" rel="noopener" href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7244682">Cooperative Capture by Multi-Agent using Reinforcement Learning, Application for Security Patrol Systems</a> by Yasuyuki S, Hirofumi O, Tadashi M, et al. Control Conference (ASCC), 2015</li>
<li><a target="_blank" rel="noopener" href="http://www4.ncsu.edu/~hdai/infocom-2015-XH.pdf">Improving learning and adaptation in security games by exploiting information asymmetry</a> by He X, Dai H, Ning P. INFOCOM, 2015.</li>
</ul>
<h4 id="7-8-Self-Play"><a href="#7-8-Self-Play" class="headerlink" title="7.8 Self-Play"></a>7.8 Self-Play</h4><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.00832.pdf">A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning</a> by Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien Perolat, David Silver, Thore Graepel. NIPS 2017.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1603.01121.pdf">Deep reinforcement learning from self-play in imperfect-information games</a> by Heinrich, Johannes, and David Silver. arXiv, 2016.</li>
<li><a target="_blank" rel="noopener" href="http://jmlr.org/proceedings/papers/v37/heinrich15.pdf">Fictitious Self-Play in Extensive-Form Games</a> by Heinrich, Johannes, Marc Lanctot, and David Silver. ICML, 2015.</li>
</ul>
<h4 id="7-9-Learning-To-Communicate"><a href="#7-9-Learning-To-Communicate" class="headerlink" title="7.9 Learning To Communicate"></a>7.9 Learning To Communicate</h4><ul>
<li><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=Hk6WhagRW">Emergent Communication through Negotiation</a> by Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls, Stephen Clark, 2018.</li>
<li><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=HJGv1Z-AW">Emergence of Linguistic Communication From Referential Games with Symbolic and Pixel Input</a> by Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, Stephen Clark</li>
<li><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=SkaxnKEYg">EMERGENCE OF LANGUAGE WITH MULTI-AGENT GAMES: LEARNING TO COMMUNICATE WITH SEQUENCES OF SYMBOLS</a> by Serhii Havrylov, Ivan Titov. ICLR Workshop, 2017.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.06585.pdf">Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning</a> by Abhishek Das, Satwik Kottur, et al. arXiv, 2017.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1703.04908.pdf">Emergence of Grounded Compositional Language in Multi-Agent Populations</a> by Igor Mordatch, Pieter Abbeel. arXiv, 2017. [<a target="_blank" rel="noopener" href="https://openai.com/blog/learning-to-communicate/">Post</a>]</li>
<li><a target="_blank" rel="noopener" href="https://repositories.lib.utexas.edu/handle/2152/45681">Cooperation and communication in multiagent deep reinforcement learning</a> by Hausknecht M J. 2017.</li>
<li><a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=Hk8N3Sclg">Multi-agent cooperation and the emergence of (natural) language</a> by Lazaridou A, Peysakhovich A, Baroni M. arXiv, 2016.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1602.02672.pdf">Learning to communicate to solve riddles with deep distributed recurrent q-networks</a> by Foerster J N, Assael Y M, de Freitas N, et al. arXiv, 2016.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1605.06676.pdf">Learning to communicate with deep multi-agent reinforcement learning</a> by Foerster J, Assael Y M, de Freitas N, et al. NIPS, 2016.</li>
<li><a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/6398-learning-multiagent-communication-with-backpropagation.pdf">Learning multiagent communication with backpropagation</a> by Sukhbaatar S, Fergus R. NIPS, 2016.</li>
<li><a target="_blank" rel="noopener" href="http://people.csail.mit.edu/lpk/papers/dars08.pdf">Efficient distributed reinforcement learning through agreement</a> by Varshavskaya P, Kaelbling L P, Rus D. Distributed Autonomous Robotic Systems, 2009.</li>
</ul>
<h4 id="7-10-Transfer-Learning"><a href="#7-10-Transfer-Learning" class="headerlink" title="7.10 Transfer Learning"></a>7.10 Transfer Learning</h4><ul>
<li><a target="_blank" rel="noopener" href="http://www.ifaamas.org/Proceedings/aamas2017/pdfs/p1100.pdf">Simultaneously Learning and Advising in Multiagent Reinforcement Learning</a> by Silva, Felipe Leno da; Glatt, Ruben; and Costa, Anna Helena Reali. AAMAS, 2017.</li>
<li><a target="_blank" rel="noopener" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14217/14005">Accelerating Multiagent Reinforcement Learning through Transfer Learning</a> by Silva, Felipe Leno da; and Costa, Anna Helena Reali. AAAI, 2017.</li>
<li><a target="_blank" rel="noopener" href="https://web.cs.umass.edu/publication/docs/2015/UM-CS-2015-004.pdf">Accelerating multi-agent reinforcement learning with dynamic co-learning</a> by Garant D, da Silva B C, Lesser V, et al. Technical report, 2015</li>
<li><a target="_blank" rel="noopener" href="https://www.scss.tcd.ie/~tayloral/res/papers/Taylor_ParallelTransferLearning_ICML_2013.pdf">Transfer learning in multi-agent systems through parallel transfer</a> by Taylor, Adam, et al. ICML, 2013.</li>
<li><a target="_blank" rel="noopener" href="https://ewrl.files.wordpress.com/2011/08/ewrl2011_submission_19.pdf">Transfer learning in multi-agent reinforcement learning domains</a> by Boutsioukis, Georgios, Ioannis Partalas, and Ioannis Vlahavas. European Workshop on Reinforcement Learning, 2011.</li>
<li><a target="_blank" rel="noopener" href="https://ai.vub.ac.be/~ydehauwe/publications/ICAART2011_2.pdf">Transfer Learning for Multi-agent Coordination</a> by Vrancx, Peter, Yann-Michaël De Hauwere, and Ann Nowé. ICAART, 2011.</li>
</ul>
<h4 id="7-11-Imitation-and-Inverse-Reinforcement-Learning"><a href="#7-11-Imitation-and-Inverse-Reinforcement-Learning" class="headerlink" title="7.11 Imitation and Inverse Reinforcement Learning"></a>7.11 Imitation and Inverse Reinforcement Learning</h4><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1907.13220">Multi-Agent Adversarial Inverse Reinforcement Learning</a> by Lantao Yu, Jiaming Song, Stefano Ermon. ICML 2019.</li>
<li><a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/7975-multi-agent-generative-adversarial-imitation-learning">Multi-Agent Generative Adversarial Imitation Learning</a> by Jiaming Song, Hongyu Ren, Dorsa Sadigh, Stefano Ermon. NeurIPS 2018.</li>
<li><a target="_blank" rel="noopener" href="http://papers.nips.cc/paper/6420-cooperative-inverse-reinforcement-learning.pdf">Cooperative inverse reinforcement learning</a> by Hadfield-Menell D, Russell S J, Abbeel P, et al. NIPS, 2016.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1403.6822.pdf">Comparison of Multi-agent and Single-agent Inverse Learning on a Simulated Soccer Example</a> by Lin X, Beling P A, Cogill R. arXiv, 2014.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1403.6508.pdf">Multi-agent inverse reinforcement learning for zero-sum games</a> by Lin X, Beling P A, Cogill R. arXiv, 2014.</li>
<li><a target="_blank" rel="noopener" href="http://aamas2014.lip6.fr/proceedings/aamas/p173.pdf">Multi-robot inverse reinforcement learning under occlusion with interactions</a> by Bogert K, Doshi P. AAMAS, 2014.</li>
<li><a target="_blank" rel="noopener" href="http://homes.soic.indiana.edu/natarasr/Papers/mairl.pdf">Multi-agent inverse reinforcement learning</a> by Natarajan S, Kunapuli G, Judah K, et al. ICMLA, 2010.</li>
</ul>
<h4 id="7-12-Meta-Learning"><a href="#7-12-Meta-Learning" class="headerlink" title="7.12 Meta Learning"></a>7.12 Meta Learning</h4><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.03641.pdf">Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments</a> by l-Shedivat, M. 2018.</li>
</ul>
<h4 id="7-13-Application"><a href="#7-13-Application" class="headerlink" title="7.13 Application"></a>7.13 Application</h4><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1712.00600.pdf">MAgent: A Many-Agent Reinforcement Learning Platform for Artificial Collective Intelligence</a> by Zheng L et al. NIPS 2017 &amp; AAAI 2018 Demo. (<a target="_blank" rel="noopener" href="https://github.com/geek-ai/MAgent">Github Page</a>)</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1702.05573.pdf">Collaborative Deep Reinforcement Learning for Joint Object Search</a> by Kong X, Xin B, Wang Y, et al. arXiv, 2017.</li>
<li><a target="_blank" rel="noopener" href="http://www.ibpsa.org/proceedings/BS2017/BS2017_051.pdf">Multi-Agent Stochastic Simulation of Occupants for Building Simulation</a> by Chapman J, Siebers P, Darren R. Building Simulation, 2017.</li>
<li><a target="_blank" rel="noopener" href="http://www.ibpsa.org/proceedings/BS2017/BS2017_056.pdf">Extending No-MASS: Multi-Agent Stochastic Simulation for Demand Response of residential appliances</a> by Sancho-Tomás A, Chapman J, Sumner M, Darren R. Building Simulation, 2017.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1610.03295.pdf">Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving</a> by Shalev-Shwartz S, Shammah S, Shashua A. arXiv, 2016.</li>
<li><a target="_blank" rel="noopener" href="https://www.researchgate.net/profile/Karl_Mason/publication/299416955_Applying_Multi-Agent_Reinforcement_Learning_to_Watershed_Management/links/56f545b908ae95e8b6d1d3ff.pdf">Applying multi-agent reinforcement learning to watershed management</a> by Mason, Karl, et al. Proceedings of the Adaptive and Learning Agents workshop at AAMAS, 2016.</li>
<li><a target="_blank" rel="noopener" href="http://www.aaai.org/ocs/index.php/AIIDE/AIIDE10/paper/viewFile/2112/2550">Crowd Simulation Via Multi-Agent Reinforcement Learning</a> by Torrey L. AAAI, 2010.</li>
<li><a target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/61bc/b98b7ae3df894f4f72aba3d145bd48ca2cd5.pdf">Traffic light control by multiagent reinforcement learning systems</a> by Bakker, Bram, et al. Interactive Collaborative Information Systems, 2010.</li>
<li><a target="_blank" rel="noopener" href="https://staff.science.uva.nl/s.a.whiteson/pubs/kuyerecml08.pdf">Multiagent reinforcement learning for urban traffic control using coordination graphs</a> by Kuyer, Lior, et al. oint European Conference on Machine Learning and Knowledge Discovery in Databases, 2008.</li>
<li><a target="_blank" rel="noopener" href="https://www.researchgate.net/publication/221465347_A_Multi-agent_Q-learning_Framework_for_Optimizing_Stock_Trading_Systems">A multi-agent Q-learning framework for optimizing stock trading systems</a> by Lee J W, Jangmin O. DEXA, 2002.</li>
<li><a target="_blank" rel="noopener" href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=422747CB9AF552CF1C4E455220E3F96F?doi=10.1.1.32.9887&amp;rep=rep1&amp;type=pdf">Multi-agent reinforcement learning for traffic light control</a> by Wiering, Marco. ICML. 2000.</li>
</ul>
<p>本文作者认为，多智能体强化学习算法在2021年前，<strong>只有QMix（基于Q值分解+DQN）和MAPPO（基于MADDPG提出的CTDE框架+PPO）这两个算法可信</strong>。其他MARL算法我只能当它们不存在。</p>
<hr>
<h3 id="8-分层强化学习-Hierarchical-RL"><a href="#8-分层强化学习-Hierarchical-RL" class="headerlink" title="8.分层强化学习 Hierarchical RL"></a>8.分层强化学习 Hierarchical RL</h3><p>神经网络有一个缺陷（特性）：在数据集A上面训练的网络，拿到数据集B上训练后，这个网络会把数据集A学到的东西忘掉（灾难性遗忘 <a href="https://link.zhihu.com/?target=https://www.sciencedirect.com/science/article/pii/S1364661399012942">Catastrophic forgetting 1999</a>）。如果我让智能体学游泳，再让它学跑步，它容易把游泳给忘了（人好像也这样，不够没有它那么严重）。深度学习领域有「迁移学习」、强化学习领域有「分层强化学习」在试图解决这些难题。</p>
<ul>
<li><strong>FuNs，分级网络 FeUdal Networks</strong> ，分层强化学习不再用单一策略去解决这些更复杂的问题，而是将策略分为上层策略与多个下层策略 sub-policy 。上层策略会根据不同的状态决定使用哪个下层策略。它使用了同策路on-policy的A3C算法</li>
<li><strong>HIRO，使用异策略进行校正的分层强化学习 HIerarchical Reinforcement learning with Off-policy correction</strong>，警惕HIRO这个算法：FuN使用同策路on-policy的A3C算法，HIRO使用异策略off-policy的TD3算法，这个让我警惕：我个人认为不能像HIRO那样去使用TD3算法。</li>
<li><strong>Option-Critic，有控制权的下层策略</strong>，让将上层的策略和下层策略的控制权也当成是可以学习的，让下层的策略学习把“决定使用哪个策略的选择权”交还给上层策略的时机，这是一种隐式的分层强化学习方案，我没有复现过这个算法，我不确定这是否真的有效。</li>
</ul>
<p>我不懂分层强化学习，请看别人写的 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46928498">张楚珩：【强化学习算法 18】FuN</a> ，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46946800">张楚珩：【强化学习算法 19】HIRO</a> ，<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47051292">张楚珩：【强化学习算法 20】Option-Critic</a></p>
<h3 id="9-逆向强化学习-Inverse-RL-与-模仿学习-Imitation-Learning"><a href="#9-逆向强化学习-Inverse-RL-与-模仿学习-Imitation-Learning" class="headerlink" title="9.逆向强化学习 Inverse RL 与 模仿学习 Imitation Learning"></a>9.逆向强化学习 Inverse RL 与 模仿学习 Imitation Learning</h3><p>强化学习会在回报函数 Reward function的指导下探索训练环境，并使用未来的期望收益来强化当前动作，试图求出更优的策略。然而，现实中不容易找到需要既懂任务又懂RL的人类去手动设计Reward function。</p>
<blockquote>
<p>以 LunarLander为例子：降落+200 坠毁-100，消耗燃料会扣0~100。其实只有这些我们也能用很长的时间训练得到能安全降落的飞行器。但实际上，我们还可以根据飞行器的平稳程度给它每步一位数的奖惩，根据飞行器距离降落点的距离给他额外的奖励。这些很细节的调整可以减少智能体的训练时间。所以我前面建议：如果训练环境 Reward function 都是初学者写的，那就用PPO。等到 Reward function 设计得更合理之后，才适合用SAC。</p>
</blockquote>
<ul>
<li>强化学习：训练环境+DRL算法+Reward Function = 搜索出好的策略</li>
<li>逆向强化学习：训练环境+IRL算法+好的策略 = 逆向得到Reward Function</li>
</ul>
<p>逆向强化学习为了解决这个问题，提出：通过模仿好的策略 去反向得到 Reward function。我不懂逆向强化学习，如果强行写解释也只能翻译其他人的综述：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1806.06877">A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress</a></p>
<h3 id="10-基于模型的强化学习算法-Model-based-RL（重点介绍MuZero）"><a href="#10-基于模型的强化学习算法-Model-based-RL（重点介绍MuZero）" class="headerlink" title="10.基于模型的强化学习算法 Model-based RL（重点介绍MuZero）"></a>10.基于模型的强化学习算法 Model-based RL（重点介绍MuZero）</h3><p>这里的「模型」指：状态转移模型。离散状态空间下的状态转移模型可以用 状态转移矩阵去描述。基于模型的算法需要将状态转移模型探索出来（或由人类提供），而 无模型算法 model-free RL 不需要探索出模型，它仅依靠智能体在环境中探索 rollout 得到的一条条 trajectory 中记录的 environment transition (s, a, r, next state) 即可对策略进行更新。</p>
<p>我对无模型算法不够了解，如果强行写解释也只能翻译其他人的综述：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2006.16712">Model-based Reinforcement Learning: A Survey</a> 。OpenAI 提供了一些简单的代码： <a href="https://link.zhihu.com/?target=https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">SpinningUp Model-based RL</a> 。</p>
<p><strong>近年来受到最多圈外人关注的 model-based RL 是 MuZero</strong>。在下棋、雅达利游戏这种状态转移模型相对容易拟合的离散动作空间任务中，MuZero取得了非常不错的表现。它有三个网络：</p>
<ul>
<li><strong>编码器：</strong>输入连续观测到的几个state，将其编码成 latent state。为何非要使用 latent state 而不直接使用 state？ 在当前state 下做出action 后，并不会转移到某个确切的状态，next state 是一个不容易直接描述的分布。因此接下来的生成器不会（也无法）直接预测 next state，只能预测 latent state。</li>
<li><strong>预测器：</strong>输入当前观测到的state，生成执行每个动作的概率，并预测执行每个动作的value （Q值，我不反对将它粗略地理解为DQN的 Q Network）。</li>
<li><strong>生成器：</strong>输入当前观测到的state，生成 执行每个离散动作后会转移到的 latent state 以及对应的 Reward（这是单步的Reward，不是累加得到的Q值）。生成器就是MuZero 这个model-based RL算法 学到的状态转移模型。</li>
</ul>
<p>如果离散动作的数量很多（如围棋），那么MuZero 会使用MCTS（<a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo tree search</a> 蒙特卡洛树搜索），剪除低概率的分支并估计Q值（论文里用 <img src="https://www.zhihu.com/equation?tex=u" alt="[公式]"> ），具体剪去多少分支要看有多少算力和时间。</p>
<p><img src="https://pic2.zhimg.com/80/v2-f11ec27c059ff5fa607351655ecfe5dd_720w.jpg" alt="img">图中左上角绿色的柱子是离散动作执行概率，可以看到：虽然围棋有很多位置可下，但实际上只有几个位置能下出好棋。如果对面不是柯洁，那么一些分支可以剪除不去计算它。</p>
<blockquote>
<p>剪枝：蒸馏学习的 Weight Pruning 权重剪枝，消除权重中不必要的值。在MCTS中的剪枝是不计算执行概率过低的动作分支。</p>
</blockquote>
<p>model-based RL 学到状态转移模型之后，就能在探索环境之前想象出接下来几步的变化，然后基于环境模型做规划，减少与环境的交互次数。（在model-based RL 中经常可以读到 Imagination，planning，dream这些词）。这里顺便解释一下 MuZero 的 Mu 是什么意思？ 解释来自<a href="https://link.zhihu.com/?target=http://www.furidamu.org/blog/2020/12/22/muzero-intuition/">MuZero Intuition - Julian Schrittwieser - What’s in a name?</a></p>
<ul>
<li>希腊字母 μ，用来表示强化学习算法学到的策略模型</li>
<li>夢（梦）。MuZero使用预测器预测智能体在环境中的下一步。</li>
<li>無（无）。MuZero（AlphaZero）不需要像前身AlphaGo 那依赖人类知识去学习。</li>
</ul>
<p>推荐看知乎问题：如何评价DeepMind新提出的MuZero算法？ 下面的 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/356976342/answer/1653420480">什么名字可以吸粉的回答</a>（他的翻译比较好：representation function 编码器，prediction function 预测器，dynamics function 生成器） 与 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/356976342/answer/906149260">Evensgn的回答</a> （Value Prediction Network 值得看一看）。</p>
<h3 id="11-会议-amp-期刊"><a href="#11-会议-amp-期刊" class="headerlink" title="11.会议&amp;期刊"></a>11.会议&amp;期刊</h3><h4 id="会议"><a href="#会议" class="headerlink" title="会议"></a>会议</h4><p>AAAI、NIPS、ICML、ICLR、IJCAI、AAMAS、IROS等</p>
<h4 id="期刊"><a href="#期刊" class="headerlink" title="期刊"></a>期刊</h4><p>AI、JMLR、JAIR、Machine Learning、JAAMAS等</p>
<h4 id="计算机和人工智能会议（期刊）排名"><a href="#计算机和人工智能会议（期刊）排名" class="headerlink" title="计算机和人工智能会议（期刊）排名"></a>计算机和人工智能会议（期刊）排名</h4><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=Mzg4MDE3OTA5NA==&amp;mid=2247490957&amp;idx=1&amp;sn=b9aa515f7833ba1503be298ac2360960&amp;source=41#wechat_redirect">清华发布新版计算机学科推荐学术会议和期刊列表，与CCF有何不同？</a><br><a target="_blank" rel="noopener" href="https://www.aminer.cn/ranks/conf/artificial-intelligence-and-pattern-recognition">https://www.aminer.cn/ranks/conf/artificial-intelligence-and-pattern-recognition</a></p>
<h3 id="12-公众号"><a href="#12-公众号" class="headerlink" title="12.公众号"></a>12.公众号</h3><p>深度强化学习实验室、机器之心、AI科技评论、新智元、学术头条</p>
<h3 id="13-知乎"><a href="#13-知乎" class="headerlink" title="13.知乎"></a>13.知乎</h3><h4 id="大牛"><a href="#大牛" class="headerlink" title="大牛"></a>大牛</h4><p>田渊栋、Flood Sung、许铁-巡洋舰科技（微信公众号同名）、<br>周博磊、俞扬、张楚珩、天津包子馅儿、JQWang2048 及其互关大牛等</p>
<h4 id="专栏"><a href="#专栏" class="headerlink" title="专栏"></a>专栏</h4><ul>
<li>David Silver强化学习公开课中文讲解及实践（叶强，比较经典）</li>
<li>强化学习知识大讲堂（《深入浅出强化学习：原理入门》作者天津包子馅儿）</li>
<li>智能单元（杜克、Floodsung、wxam，聚焦通用人工智能，Flood Sung：深度学习论文阅读路线图 Deep Learning Papers Reading Roadmap很棒，Flood Sung：最前沿：深度强化学习的强者之路）</li>
<li>深度强化学习落地方法论（西交 大牛，实操经验丰富）</li>
<li>深度强化学习（知乎：JQWang2048，GitHub：NeuronDance，CSDN：J. Q. Wang）</li>
<li>神经网络与强化学习（《Reinforcement Learning: An Introduction》读书笔记）</li>
<li>强化学习基础David Silver笔记（陈雄辉，南大，DiDi AI Labs）</li>
</ul>
<h3 id="14-官网"><a href="#14-官网" class="headerlink" title="14. 官网"></a>14. 官网</h3><h4 id="OpenAI"><a href="#OpenAI" class="headerlink" title="OpenAI"></a><a target="_blank" rel="noopener" href="http://deeprl.neurondance.com/d/www.openai.com">OpenAI</a></h4><h4 id="DeepMind"><a href="#DeepMind" class="headerlink" title="DeepMind"></a><a target="_blank" rel="noopener" href="http://deeprl.neurondance.com/d/www.deepmind.com">DeepMind</a></h4><h4 id="Berkeley-Artificial-Intelligence-Research"><a href="#Berkeley-Artificial-Intelligence-Research" class="headerlink" title="Berkeley Artificial Intelligence Research"></a><a target="_blank" rel="noopener" href="http://deeprl.neurondance.com/d/bair.berkeley.edu">Berkeley Artificial Intelligence Research</a></h4><p>以及Sutton老爷子、Andrew NG、David Silver、Pieter Abbeel、John Schulman、Sergey Levine、Chelsea Finn、Andrej Karpathy等主页</p>
<h3 id="15-环境及框架"><a href="#15-环境及框架" class="headerlink" title="15.环境及框架"></a>15.环境及框架</h3><ul>
<li><p><a target="_blank" rel="noopener" href="https://gym.openai.com/">OpenAI Gym</a> (<a target="_blank" rel="noopener" href="https://github.com/openai/gym">GitHub</a>) (<a target="_blank" rel="noopener" href="https://gym.openai.com/docs/">docs</a>)</p>
</li>
<li><p>rllab (<a target="_blank" rel="noopener" href="https://github.com/rll/rllab">GitHub</a>) (<a target="_blank" rel="noopener" href="http://rllab.readthedocs.io/">readthedocs</a>)</p>
</li>
<li><p>Ray <a target="_blank" rel="noopener" href="https://ray.readthedocs.io/en/latest/index.html">(Doc)</a></p>
</li>
<li><p>Dopamine: <a target="_blank" rel="noopener" href="https://github.com/google/dopamine">https://github.com/google/dopamine</a> (uses some tensorflow)</p>
</li>
<li><p>trfl: <a target="_blank" rel="noopener" href="https://github.com/deepmind/trfl">https://github.com/deepmind/trfl</a> (uses tensorflow)</p>
</li>
<li><p>ChainerRL (<a target="_blank" rel="noopener" href="https://github.com/chainer/chainerrl">GitHub</a>) (API: Python)</p>
</li>
<li><p>Surreal <a target="_blank" rel="noopener" href="https://github.com/SurrealAI/surreal">GitHub</a> (API: Python) (support: Stanford Vision and Learning Lab).<a target="_blank" rel="noopener" href="https://surreal.stanford.edu/img/surreal-corl2018.pdf">Paper</a></p>
</li>
<li><p>PyMARL <a target="_blank" rel="noopener" href="https://github.com/oxwhirl/pymarl">GitHub</a> (support: <a target="_blank" rel="noopener" href="http://whirl.cs.ox.ac.uk/">http://whirl.cs.ox.ac.uk/</a>)</p>
</li>
<li><p>TF-Agents: <a target="_blank" rel="noopener" href="https://github.com/tensorflow/agents">https://github.com/tensorflow/agents</a> (uses tensorflow)</p>
</li>
<li><p>TensorForce (<a target="_blank" rel="noopener" href="https://github.com/reinforceio/tensorforce">GitHub</a>) (uses tensorflow)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://sites.google.com/a/rl-community.org/rl-glue/Home/rl-glue">RL-Glue</a> (<a target="_blank" rel="noopener" href="https://code.google.com/archive/p/rl-glue-ext/wikis/RLGlueCore.wiki">Google Code Archive</a>) (API: C/C++, Java, Matlab, Python, Lisp) (support: Alberta)</p>
</li>
<li><p>MAgent <a target="_blank" rel="noopener" href="https://github.com/geek-ai/MAgent">https://github.com/geek-ai/MAgent</a> (uses tensorflow)</p>
</li>
<li><p>RLlib <a target="_blank" rel="noopener" href="http://ray.readthedocs.io/en/latest/rllib.html">http://ray.readthedocs.io/en/latest/rllib.html</a> (API: Python)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://burlap.cs.brown.edu/">http://burlap.cs.brown.edu/</a> (API: Java)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://bair.berkeley.edu/blog/2019/09/24/rlpyt/">rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/araffin/robotics-rl-srl">robotics-rl-srl</a> - S-RL Toolbox: Reinforcement Learning (RL) and State Representation Learning (SRL) for Robotics</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/deepmind/pysc2">pysc2: StarCraft II Learning Environment</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/mgbellemare/Arcade-Learning-Environment">Arcade-Learning-Environment</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/openai/universe">OpenAI universe</a> - A software platform for measuring and training an AI’s general intelligence across the world’s supply of games, websites and other applications</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/deepmind/lab">DeepMind Lab</a> - A customisable 3D platform for agent-based AI research</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/Microsoft/malmo">Project Malmo</a> - A platform for Artificial Intelligence experimentation and research built on top of Minecraft by Microsoft</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/nadavbh12/Retro-Learning-Environment">Retro Learning Environment</a> - An AI platform for reinforcement learning based on video game emulators. Currently supports SNES and Sega Genesis. Compatible with OpenAI gym.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/twitter/torch-twrl">torch-twrl</a> - A package that enables reinforcement learning in Torch by Twitter</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/facebook/UETorch">UETorch</a> - A Torch plugin for Unreal Engine 4 by Facebook</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/TorchCraft/TorchCraft">TorchCraft</a> - Connecting Torch to StarCraft</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/openai/rllab">rllab</a> - A framework for developing and evaluating reinforcement learning algorithms, fully compatible with OpenAI Gym</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/reinforceio/tensorforce">TensorForce</a> - Practical deep reinforcement learning on TensorFlow with Gitter support and OpenAI Gym/Universe/DeepMind Lab integration.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/kengz/openai_lab">OpenAI lab</a> - An experimentation system for Reinforcement Learning using OpenAI Gym, Tensorflow, and Keras.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/matthiasplappert/keras-rl">keras-rl</a> - State-of-the art deep reinforcement learning algorithms in Keras designed for compatibility with OpenAI.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://burlap.cs.brown.edu/">BURLAP</a> - Brown-UMBC Reinforcement Learning and Planning, a library written in Java</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/geek-ai/MAgent">MAgent</a> - A Platform for Many-agent Reinforcement Learning.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://ray.readthedocs.io/en/latest/rllib.html">Ray RLlib</a> - Ray RLlib is a reinforcement learning library that aims to provide both performance and composability.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/kengz/SLM-Lab">SLM Lab</a> - A research framework for Deep Reinforcement Learning using Unity, OpenAI Gym, PyTorch, Tensorflow.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/Unity-Technologies/ml-agents">Unity ML Agents</a> - Create reinforcement learning environments using the Unity Editor</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/NervanaSystems/coach">Intel Coach</a> - Coach is a python reinforcement learning research framework containing implementation of many state-of-the-art algorithms.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/ELF">ELF</a> - An End-To-End, Lightweight and Flexible Platform for Game Research</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.02627v1.pdf">Unity ML-Agents Toolkit</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/vitchyr/rlkit">rlkit</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://gym.openai.com/envs/#classic_control">https://gym.openai.com/envs/#classic_control</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/erlerobot/gym-gazebo">https://github.com/erlerobot/gym-gazebo</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/robotology/gym-ignition">https://github.com/robotology/gym-ignition</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/dartsim/gym-dart">https://github.com/dartsim/gym-dart</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/Roboy/gym-roboy">https://github.com/Roboy/gym-roboy</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/openai/retro">https://github.com/openai/retro</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/openai/gym-soccer">https://github.com/openai/gym-soccer</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/duckietown/gym-duckietown">https://github.com/duckietown/gym-duckietown</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/Unity-Technologies/ml-agents">https://github.com/Unity-Technologies/ml-agents</a> (Unity, multiagent)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/koulanurag/ma-gym">https://github.com/koulanurag/ma-gym</a> (multiagent)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/ucuapps/modelicagym">https://github.com/ucuapps/modelicagym</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/mwydmuch/ViZDoom">https://github.com/mwydmuch/ViZDoom</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/benelot/pybullet-gym">https://github.com/benelot/pybullet-gym</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/Healthcare-Robotics/assistive-gym">https://github.com/Healthcare-Robotics/assistive-gym</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/Microsoft/malmo">https://github.com/Microsoft/malmo</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/nadavbh12/Retro-Learning-Environment">https://github.com/nadavbh12/Retro-Learning-Environment</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/twitter/torch-twrl">https://github.com/twitter/torch-twrl</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/arex18/rocket-lander">https://github.com/arex18/rocket-lander</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/ppaquette/gym-doom">https://github.com/ppaquette/gym-doom</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/thedimlebowski/Trading-Gym">https://github.com/thedimlebowski/Trading-Gym</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/Phylliade/awesome-openai-gym-environments">https://github.com/Phylliade/awesome-openai-gym-environments</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/deepmind/pysc2">https://github.com/deepmind/pysc2</a> (by DeepMind) (Blizzard StarCraft II Learning Environment (SC2LE) component)</p>
</li>
</ul>
<h3 id="好用的强化学习算法"><a href="#好用的强化学习算法" class="headerlink" title="好用的强化学习算法"></a>好用的强化学习算法</h3><ul>
<li>没有很多需要调整的超参数。D3QN、SAC超参数较少，且SAC可自行调整超参数 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"></li>
<li>超参数很容易调整或确定。SAC的 reward scaling 可以在训练前直接推算出来。PPO超参数的细微改变不会极大地影响训练</li>
<li>训练快，收敛稳、得分高。看下面的学习曲线 learning curve</li>
</ul>
<p><img src="https://pic4.zhimg.com/80/v2-9d8ff9652e255dbf6d41e754a9b0d997_720w.jpg" alt="img">弯弯曲曲的学习曲线很正常，图片截取自 Ape-X 与 SAC 论文</p>
<h4 id="学习曲线怎么看？"><a href="#学习曲线怎么看？" class="headerlink" title="学习曲线怎么看？"></a>学习曲线怎么看？</h4><ul>
<li>横轴可以是训练所需的步数（智能体与环境交互的次数）、训练轮数（达到固定步数、失败、通关 就终止终止这一轮的训练episode）、训练耗时（这个指标还与设备性能有关）</li>
<li>纵轴可以是 每轮得分（ 每一轮的每一步的reward 加起来，episode return），对于没有终止状态的任务，可以计算某个时间窗口内reward之和</li>
<li>有时候还有用 plt.fill_between 之类的上下std画出来的波动范围，用于让崎岖的曲线更好看一点：先选择某一段数据，然后计算它的均值，再把它的标准差画出来，甚至可以画出它的上下偏差（琴形图）。如果同一个策略在环境随机重置后得分相差很大，那么就需要多测几次。</li>
</ul>
<h4 id="好的算法的学习曲线应该是？"><a href="#好的算法的学习曲线应该是？" class="headerlink" title="好的算法的学习曲线应该是？"></a>好的算法的学习曲线应该是？</h4><ul>
<li>训练快，曲线越快达到某个目标分数 target reward （需要多测几次的结果才有说服力）</li>
<li>收敛稳，曲线后期不抖动（曲线在前期剧烈抖动是可以接受的）</li>
<li>得分高，曲线的最高点可以达到很高（即便曲线后期下降地很厉害也没关系，因为我们可以保存整个训练期间“平均得分”最高的模型）</li>
</ul>
<h2 id="未来的工作方向"><a href="#未来的工作方向" class="headerlink" title="未来的工作方向"></a>未来的工作方向</h2><p>所以当下有两条路建议选择：一是去腾讯AIlab、华为诺亚、网易伏羲或者阿里研究型项目，二是去其他高校有积淀的实验室。这两种方式都蛮不错的，应该会比自己闭门造车的速度快。只不过，这些地方的要求都不会低，想要去做科研要好好准备面试。</p>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">微笑紫瞳星</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://tianjuewudi.gitee.io/2021/09/05/ge-ren-xue-xi-fang-xiang-ji-yan-jiu-fang-xiang/">https://tianjuewudi.gitee.io/2021/09/05/ge-ren-xue-xi-fang-xiang-ji-yan-jiu-fang-xiang/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">微笑紫瞳星</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95/">
                                    <span class="chip bg-color">研究方法</span>
                                </a>
                            
                                <a href="/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/">
                                    <span class="chip bg-color">论文写作</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    
        <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments textarea {
        box-sizing: border-box;
        background: url("/medias/comment_bg.png") 100% 100% no-repeat;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #4cbf30;
        font-weight: 500;
        text-decoration: none;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="vcomments" class="card-content" style="display: grid">
    </div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<script>
    new Valine({
        el: '#vcomments',
        appId: 'bpzS5Q9Q9zMl4doyyDGqIqqq-gzGzoHsz',
        appKey: 'M0IfLJW2G1dCMyLTb90tFxT8',
        notify: 'true' === 'true',
        verify: 'false' === 'true',
        visitor: 'true' === 'true',
        avatar: 'mm',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: '请畅所欲言'
    });
</script>

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2021/09/08/qiang-hua-xue-xi-zhi-ppo/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/6.jpg" class="responsive-img" alt="强化学习之PPO">
                        
                        <span class="card-title">强化学习之PPO</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-09-08
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    强化学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">强化学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/09/03/qiang-hua-xue-xi-zhi-trpo/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/22.jpg" class="responsive-img" alt="强化学习之TRPO">
                        
                        <span class="card-title">强化学习之TRPO</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-09-03
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" class="post-category">
                                    强化学习
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">
                        <span class="chip bg-color">强化学习</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('50')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: 微笑紫瞳星<br />'
            + '文章作者: 微笑紫瞳星<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处，谢谢啦~';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="tencent"
                   type="playlist"
                   id="8075110837"
                   fixed='true'
                   autoplay='true'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.35'
                   list-folded='false'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2021</span>
            
            <span id="year">2021</span>
            <a href="/about" target="_blank">微笑紫瞳星</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">210.9k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <span id="sitetime">载入运行时间...</span>
            <script>
                function siteTime() {
                    var seconds = 1000;
                    var minutes = seconds * 60;
                    var hours = minutes * 60;
                    var days = hours * 24;
                    var years = days * 365;
                    var today = new Date();
                    var startYear = "2021";
                    var startMonth = "1";
                    var startDate = "9";
                    var startHour = "0";
                    var startMinute = "0";
                    var startSecond = "11";
                    var todayYear = today.getFullYear();
                    var todayMonth = today.getMonth() + 1;
                    var todayDate = today.getDate();
                    var todayHour = today.getHours();
                    var todayMinute = today.getMinutes();
                    var todaySecond = today.getSeconds();
                    var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                    var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                    var diff = t2 - t1;
                    var diffYears = Math.floor(diff / years);
                    var diffDays = Math.floor((diff / days) - diffYears * 365);
                    var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
                    var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) /
                        minutes);
                    var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours -
                        diffMinutes * minutes) / seconds);
                    if (startYear == todayYear) {
                        document.getElementById("year").innerHTML = todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffDays + " 天 " + diffHours +
                            " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    } else {
                        document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                        document.getElementById("sitetime").innerHTML = "本站已安全运行 " + diffYears + " 年 " + diffDays +
                            " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
                    }
                }
                setInterval(siteTime, 1000);
            </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">








    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1121452406" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1121452406" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":200,"height":400},"mobile":{"show":false},"log":false});</script></body>

</html>
