<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="个人学习方向及研究方向, 博客制作,个人经验分享,Unity,人工智能等">
    <meta name="description" content="本站记录本人各种学习的旅途，用于巩固自我并启发后来人">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>个人学习方向及研究方向 | 微笑紫瞳星</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>



   <style>
    body{
       background-image: url(https://cdn.jsdelivr.net/gh/Tokisaki-Galaxy/res/site/medias/background.jpg);
       background-repeat:no-repeat;
       background-size:cover;
       background-attachment:fixed;
    }
</style>



<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">微笑紫瞳星</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">微笑紫瞳星</div>
        <div class="logo-desc">
            
            本站记录本人各种学习的旅途，用于巩固自我并启发后来人
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/18.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">个人学习方向及研究方向</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95/">
                                <span class="chip bg-color">研究方法</span>
                            </a>
                        
                            <a href="/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/">
                                <span class="chip bg-color">论文写作</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/" class="post-category">
                                学习方法
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-09-05
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2021-09-05
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    30 分
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        

        
        <!-- 代码块折行 -->
        <style type="text/css">
            code[class*="language-"], pre[class*="language-"] { white-space: pre-wrap !important; }
        </style>
        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <p> 2021年9月5日更新：</p>
<h2 id="深度强化学习研究方向概述"><a href="#深度强化学习研究方向概述" class="headerlink" title="深度强化学习研究方向概述"></a>深度强化学习研究方向概述</h2><p>注：本段内容摘自知乎<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/342919579">https://zhuanlan.zhihu.com/p/342919579</a></p>
<p><strong>赶时间请直接看加粗的四种算法</strong>，它们占据不同的生态位，请根据实际任务需要去选择他们，在强化学习的子领域（多智能体、分层强化学习、逆向强化学习也会以它们为基础开发新的算法）：</p>
<ul>
<li>离散动作空间推荐：<strong>Dueling DoubleQN（D3QN）</strong></li>
<li>连续动作空间推荐：擅长调参就用<strong>TD3</strong>，不擅长调参就用<strong>PPO或SAC</strong>，如果训练环境 Reward function 都是初学者写的，那就用PPO</li>
</ul>
<p>来自另外一个大佬的建议：对于连续控制任务，推荐SAC、TD3和PPO，三种算法都值得试一试并从中择优；对于离散控制任务，推荐SAC-Discrete（即离散版SAC）和PPO。</p>
<h3 id="离散的动作空间-discrete-action-space"><a href="#离散的动作空间-discrete-action-space" class="headerlink" title="离散的动作空间 discrete action space"></a>离散的动作空间 discrete action space</h3><ul>
<li><p><strong>DQN（Deep Q Network）</strong>可用于入门深度强化学习，使用一个Q Network来估计Q值，从而替换了 Q-table，完成从离散状态空间到连续状态空间的跨越。Q Network 会对每一个离散动作的Q值进行估计，执行的时候选择Q值最高的动作（greedy 策略）。并使用 epslion-greedy 策略进行探索（探索的时候，有很小的概率随机执行动作），来获得各种动作的训练数据。</p>
</li>
<li><p><strong>DDQN（Double DQN）</strong>更加稳定，因为最优化操作会传播高估误差，所以她同时训练两个Q network并选择较小的Q值用于计算TD-error，降低高估误差。详见和DDQN一样采用了两个估值网络的TD3 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86297106">曾伊言：强化学习算法TD3论文的翻译与解读</a></p>
</li>
<li><p><strong><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1511.06581">Dueling DQN</a>**，Dueling DQN 使用了优势函数 advantage function（A3C也用了）：它只估计state的Q值，不考虑动作，好的策略能将state 导向一个更有优势的局面。原本DQN对一个state的Q值进行估计时，它需要等到</strong>为每个离散动作收集到数据后<strong>，才能进行准确估值。然而，在某些state下，采取不同的action并不会对Q值造成多大的影响，因此Dueling DQN 结合了 优势函数估计的Q值 与 原本DQN对不同动作估计的Q值。使得在某些state下，Dueling DQN 能在</strong>只收集到一个离散动作的数据后**，直接得到准确的估值。当某些环境中，存在大量不受action影响的state，此时Dueling DQN能学得比DQN更快。</p>
</li>
<li><p><strong>D3QN（Dueling Double DQN）。</strong>Dueling DQN 与Double DQN 相互兼容，一起用效果很好。简单，泛用，没有使用禁忌。任何一个刚入门的人都能独立地在前两种算法的基础上改出D3QN。在论文中使用了D3QN应该引用DuelingDQN 与 DoubleDQN的文章。</p>
</li>
<li><p><strong>Noisy DQN</strong>，探索能力稍强。Noisy DQN 把噪声添加到网络的输出层之前值。原本Q值较大的动作在添加噪声后Q值变大的概率也比较大。这种探索比epslion-greedy随机选一个动作去执行更好，至少这种针对性的探索既保证了探索动作多样，也提高了探索效率。</p>
</li>
</ul>
<p>====估计Q值的期望↑ ↓估计Q值的分布====</p>
<ul>
<li><strong>Distributional RL 值分布RL（C51，Distributional Perspective RL）</strong>。在DQN中，Q Network 拟合了Q值的期望，期望可以用一个数值去描述，比较简单。在值分布DQN中，Q Network 拟合了Q值的分布，Q值分布的描述就要麻烦一些了，但是训练效果更好。为C51的算法使用了这种方法，C表示Categorical，51表示他们将值分布划分51个grid。最终在雅达利游戏 Atari Game 上取得好结果。</li>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1710.10044.pdf">QR-DQN</a><strong>（分位数回归</strong> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40681570">Quantile Regression</a><strong>）</strong>，使用N个分位数去描述Q值分布（这种方法比C51划分51个grid的方法更妙，我推荐看 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/138091493">QR-DQN - Frank Tian</a>）。根据分位数的分布画出核分布曲线，详见 <a href="https://link.zhihu.com/?target=https://aakinshin.net/posts/qrde-hd/">Quantile-respectful density estimation based on the Harrell-Davis quantile estimator</a></li>
<li><strong>Rainbow DQN</strong>，上面提及的DQN变体很多是相互兼容的，因此 David Sliver 他们整合了这些变体，称为Rainbow。</li>
<li><strong>Ape-X DQN（Distributed Prioritized Experience Replay）</strong>，也是 David Sliver 他们做的。使用了Distributed training，用多个进程创建了多个actor去与环境交互，然后使用收集到的数据去训练同一个learner，用来加快训练速度。Prioritized Experience Replay（优先经验回放 PER 下面会讲）。Ape-X通过充分利用CPU资源，合理利用GPU，从而加快了训练速度。注意，这不等同于减少训练总步数。NVIDIA 有一个叫 Apex的库，用于加速计算。</li>
</ul>
<hr>
<h3 id="连续的动作空间-continuous-action-space"><a href="#连续的动作空间-continuous-action-space" class="headerlink" title="连续的动作空间 continuous action space"></a>连续的动作空间 continuous action space</h3><ul>
<li><strong>DDPG（Deep DPG ）</strong>，可用于入门连续动作空间的DRL算法。DPG 确定策略梯度算法，直接让策略网络输出action，成功在连续动作空间任务上训练出能用的策略，但是它使用 OU-noise 这种有很多超参数的方法去探索环境，训练慢，且不稳定。</li>
<li><strong>soft target update（软更新）</strong>，用来稳定训练的方法，非常好用，公式是 <img src="https://www.zhihu.com/equation?tex=%5Ctheta%27+=+%5Ctau+%5Ctheta%27+++(1-%5Ctau)%5Ctheta" alt="[公式]"> ，其中 theta是使用梯度进行更新的网络参数，theta’ 是使用了软更新的目标网络target network参数，tau略小于1。软更新让参数的更新不至于发生剧变，从而稳定了训练。从DDPG开始就广泛使用，并且在深度学习的其他领域也能看到它的身影，如 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/159765213">谷歌自监督 BYOL Bootstrap Your Own Latent</a> ，看论文的公式（1），就用了soft target update</li>
<li><strong>TD3（TDDD，Twin Delay DDPG）</strong>，擅长调参的人才建议用，因为它影响训练的敏感超参数很多。它从Double DQN那里继承了Twin Critic，用来降低高估误差；它用来和随机策略梯度很像的方法：计算用于更新TD-error的Q值时，给action加上了噪声，用于让Critic拟合更平滑的Q值估计函数。TD3建议 延迟更新目标网络，即多更新几次网络后，再使用 soft update 将网络更新到target network上，我认为这没有多大用，后来的其他算法也不用这个技巧。TD3还建议在计算Q值时，为动作添加一个噪声，用于平滑Critic函数，在确定策略中，TD3这么用很像“随机策略”。详见 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86297106">曾伊言：强化学习算法TD3论文的翻译与解读</a></li>
<li><strong>D4PG（Distributed Distributional DDPG）</strong>，这篇文章做了实验，证明了一些大家都知道好用的trick是好用的。Distributed：它像 Ape-X一样用了 多线程开了actors 加快训练速度，Distributional：Q值分布RL（看前面的C51、QR-DQN）。DDPG探索能力差的特点，它也完好无缺地继承了。</li>
</ul>
<blockquote>
<p>在C51算法论文标题中，Distributional Perspective 指 Q值分布的表示。在Ape-X DQN 标题中， Distributed training 指分布式训练。我曾混淆过它们。</p>
</blockquote>
<p>====确定策略梯度↑ ↓ 随机策略梯度====</p>
<ul>
<li><strong>Stochastic Policy Gradient 随机策略梯度</strong>，随机策略的探索能力更好。随机策略网络会输出action的分布（通常输出高斯分布 均值 与 方差，少数任务下用其他分布），探索的噪声大小由智能体自己决定，更加灵活。但是这对算法提出了更高的要求。</li>
<li><strong>A3C（Asynchronous Advantage Actor-Critic）</strong>，Asynchronous 指开启多个actor 在环境中探索，并异步更新。原本DDPG的Critic 是 Q(s, a)，根据state-action pair 估计Q值，优势函数只使用 state 去估计Q值，<strong>这是很好的创新：降低了随机策略梯度算法估计Q值的难度</strong>。<strong>然而优势函数有明显缺陷</strong>：不是任何时刻 action 都会影响 state的转移（详见 Dueling DQN），因此这个算法只适合入门学习「优势函数 advantage function」。如果你看到新论文还在使用A3C，那么你要怀疑其作者RL的水平。此外，A3C算法有离散动作版本，也有连续动作版本。A2C 指的是没有Asynchronous 的版本。</li>
<li><strong>TRPO（Trust Region Policy Optimization）</strong>，信任域 Trust Region。连续动作空间无法每一个动作都搜索一遍，因此大部分情况下只能靠猜。如果要猜，就只能在信任域内部去猜。TRPO将每一次对策略的更新都限制了信任域内，从而极大地增强了训练的稳定性。可惜信任域的计算量太大了，因此其作者推出了PPO，如果你PPO论文看不懂，那么建议你先看TRPO。如果你看到新论文还在使用TRPO，那么你要怀疑其作者RL的水平。</li>
<li><strong>PPO（Proximal PO 近端策略搜索）</strong>，训练稳定，调参简单，robust（稳健、耐操）。PPO对TRPO的信任域计算过程进行简化，论文中用的词是 surrogate objective。PPO动作的噪声方差是一个可训练的矢量（与动作矢量相同形状），而不由网络输出，这样做增强了PPO的稳健性 robustness。</li>
<li><strong>PPO+GAE（</strong><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1506.02438">Generalized Advantage Estimation</a><strong>）</strong>，训练最稳定，调参最简单，适合高维状态 High-dimensional state，<strong>但是环境不能有太多随机因数</strong>。GAE会根据经验轨迹 trajectory 生成优势函数估计值，然后让Critic去拟合这个值。在这样的调整下，在随机因素小的环境中，不需要太多 trajectory 即可描述当前的策略。尽管GAE可以用于多种RL算法，但是她与PPO这种On-policy 的相性最好。</li>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2009.04416">PPG</a><strong>（Proximal Policy Gradient）</strong>，A3C、PPO 都是同策略 On-policy，它要求：在环境中探索并产生训练数据的策略 与 被更新的策略网络 一定得是同一个策略。她们需要删掉已旧策略的数据，然后使用新策略在环境中重新收集。为了让PPO也能用 off-policy 的数据来训练，PPG诞生了，思路挺简单的，原本的On-policy PPO部分该干啥干啥，额外引入一个使用off-policy数据进行训练的Critic，让它与PPO的Critic共享参数，也就是Auxiliary Task，参见 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/342150033">Flood Sung：深度解读：Policy Gradient，PPO及PPG</a> ，以及<a href="https://link.zhihu.com/?target=https://zhuanlan.zhi%3Cb%3Ehu.com/p/14851%3C/b%3E6171">白辰甲：强化学习中自适应的辅助任务加权(Adaptive Auxiliary Task Weighting)</a>。这种算法并不是在任何情况下都能比PPO好，因为PPG涉及到Auxiliary task，这要求她尽可能收集更多的训练数据，并在大batch size 下面才能表现得更好。</li>
<li>Interpolated Policy Gradient NIPS.2017，<strong>反面例子</strong>，它试图基于 On-policy TRPO 改出一个 能利用 off-policy 数据的TRPO来（就像PPO→PPG），然而Interpolated Policy Gradient 强行地、错误地使用off-policy 数据对Critic 进行训练。结果在其论文中放出的结果中，它的性能甚至比A3C还差，只是比TRPO、DDPG略好（但是它故意没有和比它好的算法在同一个任务下比较：论文结果很诚实，但是用事实说谎）。</li>
<li>Soft Q-learning（Deep Energy Based Policy）是SAC的前身，最大熵算法的萌芽，她的作者后来写出了SAC（都叫soft ***），你可以跳过Soft QL，直接看SAC的论文。<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/76681229">黄伟：Soft Q-Learning论文阅读笔记</a></li>
<li><strong>SAC（Soft Actor-Critic with maximum entropy 最大熵）</strong>，训练很快，探索能力好，但是很依赖Reward Function，不像PPO那样随便整一个Reward function 也能训练。PPO算法会计算新旧策略的差异（计算两个分布之间的距离），并让这个差异保持在信任域内，且不至于太小。SAC算法不是on-policy算法，不容易计算新旧策略的差异，所以它在优化时最大化策略的熵（动作的方差越大，策略的熵越高）。</li>
<li><strong>SAC（Automating Entropy Adjustment/ Automating Temperature Parameter</strong> <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> <strong>自动调整温度系数并维持策略的熵在某个值附近）</strong>一般我们使用的SAC是这个版本的SAC，它能自动调整一个叫温度系数alpha 的超参数（温度越高，熵越大）。SAC的策略网络的优化目标=累计收益+ alpha*策略的熵。一般在训练后期，策略找到合适的action分布均值时，它的action分布方差越小，其收益越高，因而对“累计收益”进行优化，会让策略熵倾向于减小。SAC会自动选择合适的温度系数，让策略的熵保持一种适合训练的动态平衡。SAC会事先确定一个目标熵 target entropy（论文作者的推荐值是 log(action_dim)），如果策略熵大于此值，则将alpha调小，反之亦然。从这个角度看，SAC就不是最大化策略熵了，而是将策略熵限制在某个合适大小内，这点又与PPO的“保持在信任域内，且不至于太小”不谋而合</li>
</ul>
<hr>
<h3 id="混合的动作空间-hybrid-action-space"><a href="#混合的动作空间-hybrid-action-space" class="headerlink" title="混合的动作空间 hybrid action space"></a>混合的动作空间 hybrid action space</h3><p>在实际任务中，混合动作的需求经常出现：如王者荣耀游戏既需要离散动作（选择技能），又需要连续动作（移动角色）。只要入门了强化学习，就很容易独立地想出以下这些方法，所以我没有把它们放在前面：</p>
<ul>
<li><strong>强行使用DQN类算法，把连续动作分成多个离散动作：</strong>不建议这么做，这破坏了连续动作的优势。一个良性的神经网络会是一个平滑的函数（k-Lipschitz 连续），相近的输入会有相似的输出。在连续的动作空间开区间[-1, +1]中，智能体会在学了-1，+1两个样本后，猜测0的样本可能介于 -1，+1 之间。而强行将拆分为离散动作 -1，0，+1之后（无论拆分多么精细），它都猜不出 0的样本，一定要收集到 0的样本才能学习。此外，精细的拆分会增加离散动作个数，尽管更加逼近连续动作，但会增加训练成本。</li>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1910.07207">SAC for Discrete Action Space</a><strong>，把输出的连续动作当成是离散动作的执行概率：</strong>SAC for Discrete Action Space 这个算法提供了将连续动作算法SAC应用在离散动作的一条技术路线：把这个输出的动作矢量当成每个动作的执行概率。一般可以直接把离散动作部分全部改成连续动作，然后套用连续动作算法，这方法简单，但是不一定最好的。</li>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1810.06394">P-DQN</a> <strong>（Parameterized DQN），把DQN和DDPG合起来</strong>：Q network 会输出每个动作对应的Q值，执行的时候选择Q值高的动作。DDPG与其他策略梯度算法，让Critic预测 state-action的Q值，然后用Critic 提供的梯度去优化Actor，让Actor输出Q值高的动作。现在，对于一个混合动作来说，我们可以让Critic学习Q Network，让Critic也为每个离散动作输出对应的Q值，然后用Critic中 arg max Qi 提供梯度优化Actor。这是很容易独立想出来的方法，相比前两个方案缺陷更小。</li>
<li><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1903.01344">H-PPO</a> <strong>（Hybrid PPO），同时让策略网络输出混合动作</strong>。连续动作（策略梯度）算法中：DDPG、TD3、SAC使用 状态-动作值函数 Q(state, action)，A3C、<strong>PPO使用 状态值函数 Q(state)**。离散动作无法像连续动作一样将一个action输入到 Q(state, action) 里，因此 Hybird PPO选择了PPO。于是它的策略网络会像Q Network 一样为离散动作输出不同的Q值，也像PPO 一样输出连续动作。（</strong>警告，2021-03 前它依然没有开源代码，但论文描述的方法无