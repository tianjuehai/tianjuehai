<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>分布式强化学习</title>
      <link href="2021/10/10/fen-bu-shi-qiang-hua-xue-xi/"/>
      <url>2021/10/10/fen-bu-shi-qiang-hua-xue-xi/</url>
      
        <content type="html"><![CDATA[<p> 阅读本文需要强化学习基础，可以阅读我以前的文章：<a href="https://tianjuewudi.gitee.io/2021/07/21/qiang-hua-xue-xi-gang-yao-zhou-bo-lei-ke-cheng/">强化学习纲要（周博磊课程）</a>、<a href="https://tianjuewudi.gitee.io/2021/07/16/qiang-hua-xue-xi-shi-jian-jiao-xue/">强化学习实践教学</a></p><h2 id="分布式系统"><a href="#分布式系统" class="headerlink" title="分布式系统"></a>分布式系统</h2><p>一般情况下我们做的论文课题都是小规模的，使用的都是一个相对较小的数据库，因此使用单机系统基本可以完成任务。但现实生活中的数据往往是巨量的，我们需要一个完整的分布式系统来处理这种大规模的数据。算法和结果只是冰山一角，只有拥有一个好的系统和框架作为支撑，才能得到好的算法和实验结果。</p><p>分布式系统需要满足：</p><ul><li>一致性：确保多节点的协调运作并且结果和单机运行的结果一致。</li><li>容错性：当分布式环境工作时，其中某一个节点出现错误（机器宕机等），任务能够分配到其他机器，工作也能正常。</li><li>交流：分布式系统需要I\O和分布式文件系统的知识。</li></ul><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/1.png" class=""><p>分布式系统中存在分布式学习的模型和数据，因此有两种并行。一种是不同的机器做一个网络不同部分的计算。第二种是一套机器拥有一个单独的模型拷贝，但是分配到的数据是不同的，计算结果汇总。</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/2.png" class=""><p>一台电脑可以拥有多块显卡，因此每一块显卡都可以负责模型的一部分，每台电脑都可以放置一个模型，然后把数据分配到每台电脑的每一块显卡上，这就是上面两种并行方法的综合运用。</p><hr><p>使用算法和模型的时候需要在机器之间传输信息，怎么实施信息之间的交互是需要解决的问题。一种更新参数的方法是使用<strong>Parameter Server</strong>。我们可以用另外一台机器接收各个机器传回来的模型参数，然后给这些参数取平均得到更新值，然后把更新后的参数返回给各个机器，使得每一台机器都可以保持相同的参数进行分布式计算。这里我们也可以在各个机器中计算梯度后只把梯度传递回主机，主机中乘上一个学习率即可。</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/3.png" class=""><hr><p>对于<strong>模型的更新</strong>有两种常见的方法：同步更新和异步更新。</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/4.png" class=""><hr><p>上面的方法需要一个主机Parameter Sever，如果主机出现错误，整个训练就会失败。因此我们可以不用主机，这也叫做分散异步随机梯度下降，机器中间点对点传输梯度来更新参数。</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/5.png" class=""><hr><p>分布式优化：asych SGD，不加lock，这样会导致一个进程读取的参数被另一个进程抢先更新的情况。但Hogwild给出证明两种算法在一定情况下结果趋紧一致。asych SGD是并行系统中广泛应用的设计。</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/6.png" class=""><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>multiprocessing <span class="token keyword">as</span> mp<span class="token keyword">from</span> model <span class="token keyword">import</span> MyModel<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># Construct data_loader,optimizer,etc.</span>    <span class="token keyword">for</span> data<span class="token punctuation">,</span>labels <span class="token keyword">in</span> data_loader<span class="token punctuation">:</span>        optimizer<span class="token punctuation">.</span>zero_frad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss_fn<span class="token punctuation">(</span>model<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">,</span>labels<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        opeimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># update the shared parameters</span> <span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    num_processes <span class="token operator">=</span> <span class="token number">4</span>    model <span class="token operator">=</span> MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># NOTE: this is required for the "fork" method to work</span>    model<span class="token punctuation">.</span>share_memory<span class="token punctuation">(</span><span class="token punctuation">)</span>     processes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> rank <span class="token keyword">in</span> range<span class="token punctuation">(</span>num_processes<span class="token punctuation">)</span><span class="token punctuation">:</span>        p <span class="token operator">=</span> mp<span class="token punctuation">.</span>Process<span class="token punctuation">(</span>target<span class="token operator">-</span>train<span class="token punctuation">,</span>args<span class="token operator">=</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        p<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">for</span> p <span class="token keyword">in</span> processes<span class="token punctuation">:</span>        p<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><hr><p>MapReduce：分布式学习的开山鼻祖算法</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/7.png" class=""><hr><p>DisBelief</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/8.png" class=""><hr><p>AlexNet</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/9.png" class=""><hr><h2 id="分布式强化学习系统"><a href="#分布式强化学习系统" class="headerlink" title="分布式强化学习系统"></a>分布式强化学习系统</h2><p>现在我们考虑强化学习系统的哪些部分可以实行分布式设计。</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/10.png" class=""><p>我们可以从以上三点出发：考虑增加更多的环境，让更多的智能体同时工作，并且让多个episode同时执行。</p><p>在分布式强化学习之中，我们需要多个环境，多个智能体进行交互。因此在不同的机器中都可以有一个环境和一个智能体，得到多个轨迹，然后传回给learner，learner对参数进行更新，然后把更新后的参数传回给不同机器中的智能体。</p><h3 id="经典算法"><a href="#经典算法" class="headerlink" title="经典算法"></a>经典算法</h3><p>分布式强化学习系统的进展：</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/11.png" class=""><p>GORILA系统是在DQN的基础上进行分布式加速：</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/12.png" class=""><p><strong>A3C</strong></p><p>A3C相对于GORILA取缔了reply memory，每个线程都保留了自己的actor，因此每个线程的轨迹都是具有多样化的，可以直接采样进行学习。</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/13.png" class=""><p>部分代码：</p><pre class=" language-python"><code class="language-python">processes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> rank <span class="token keyword">in</span> range<span class="token punctuation">(</span>args<span class="token punctuation">.</span>processes<span class="token punctuation">)</span><span class="token punctuation">:</span>    p <span class="token operator">=</span> mp<span class="token punctuation">.</span>Process<span class="token punctuation">(</span>target<span class="token operator">=</span>train<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span>shared_model<span class="token punctuation">,</span> shared_optimizer<span class="token punctuation">,</span> rank<span class="token punctuation">,</span> args<span class="token punctuation">,</span> info<span class="token punctuation">)</span><span class="token punctuation">)</span>    p<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>    processes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token keyword">for</span> p <span class="token keyword">in</span> processes<span class="token punctuation">:</span>    p<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><p><strong>A2C</strong>：基于A3C的改进</p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/14.png" class=""><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/15.png" class=""><p><strong>Apex-X</strong></p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/16.png" class=""><p><strong>IMPALA</strong></p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/17.png" class=""><p><strong>RLlib</strong></p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/18.png" class=""><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/19.png" class=""><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/20.png" class=""><p><strong>Evolution Strategies</strong></p><img src="/2021/10/10/fen-bu-shi-qiang-hua-xue-xi/21.png" class="">]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模仿学习（Imitation Learning）</title>
      <link href="2021/10/10/mo-fang-xue-xi-imitation-learning/"/>
      <url>2021/10/10/mo-fang-xue-xi-imitation-learning/</url>
      
        <content type="html"><![CDATA[<p>在游戏中，我们往往有一个计分板准确定义事情的好坏程度。但现实中，定义Reward有可能是非常困难的，并且人定的reward也有可能存在许多意想不到的缺陷。在没有reward的情况下，让AI跟环境互动的一个方法叫做<strong>Imitation-Learning</strong>。在没有reward的前提下，我们可以找人类进行示范，AI可以凭借这些示范以及跟环境的互动进行学习。这种模仿学习使得智能体自身不必从零学起，不必去尝试探索和收集众多的无用数据，能大大加快训练进程。</p><p>这跟supervised-learning有类似之处，如果采用这种做法，我们叫做Behavior-Cloning，也就是复制人类的行为。</p><p>但是这种监督学习有一个缺点，如果是智能体进入到一个以前从来没有见到过的状态，就会产生较大的误差，这种误差会一直累加，到最后没有办法进行正常的行为。因此我们需要让实际遇到的数据和训练数据的分布尽量保持一致。</p><h2 id="DAgger"><a href="#DAgger" class="headerlink" title="DAgger"></a>DAgger</h2><p>前期先让人类去操作policy，拿到足够多的数据以后做完全意义上的offline训练；如果offline训出来效果不好，把效果不好的场景再让人类操作一遍，对各个状态打上动作标签。然后新数据加旧数据一起再训练，直到效果变好为止，这就是DAgger。</p><img src="/2021/10/10/mo-fang-xue-xi-imitation-learning/1.png" class=""><p>通常第三步人来收集数据也是一个比较麻烦和漫长的过程，我们也可以使用其他算法来代替人类来打标签。</p><h2 id="逆强化学习（Inverse-Reinforcement-Learning）"><a href="#逆强化学习（Inverse-Reinforcement-Learning）" class="headerlink" title="逆强化学习（Inverse Reinforcement Learning）"></a>逆强化学习（Inverse Reinforcement Learning）</h2><p>在强化学习中，我们给定环境（状态转移）和奖励函数，我们需要通过收集的数据来对自身的策略函数和值函数进行优化。在逆强化学习中，提供环境（状态转移），也提供策略函数或是示教数据，我们希望从这些数据中反推奖励函数。即给定状态和动作，建立模型输出对应奖励。在奖励函数建立好后，我们就能新训练一个智能体来模仿给定策略（示教数据）的行为。</p><h3 id="GAIL（Generative-Adversarial-Imitation-Learning）"><a href="#GAIL（Generative-Adversarial-Imitation-Learning）" class="headerlink" title="GAIL（Generative Adversarial Imitation Learning）"></a>GAIL（Generative Adversarial Imitation Learning）</h3><img src="/2021/10/10/mo-fang-xue-xi-imitation-learning/2.png" class=""><p>在IRL领域有名的算法是GAIL，这种算法模仿了生成对抗网络GANs。把Actor当成Generator，把Reward Funciton当成Discriminator。<br>我们要训练一个策略网络去尽量拟合提供的示教数据，那么我们可以让需要训练的reward函数来进行评价，Reward函数通过输出评分来分辨哪个是示教数据的轨迹，哪个是自己生成的虚假轨迹；而策略网络负责生成虚假的轨迹，尽可能骗过Reward函数，让其难辨真假。两者是对抗关系，双方的Loss函数是对立的，两者在相互对抗中一起成长，最后训练出一个较好的reward函数和一个较好的策略网络。</p><h2 id="模仿学习结合强化学习"><a href="#模仿学习结合强化学习" class="headerlink" title="模仿学习结合强化学习"></a>模仿学习结合强化学习</h2><p>模仿学习的特点：</p><ol><li>用人工收集数据往往需要较大成本，而且数据量也不会很大，并且存在数据分布不一致的问题。</li><li>人也有很多办不到的策略，如果是非常复杂的控制（例如高达机器人，六旋翼飞行器），人是没办法胜任的。</li><li>训练稳定简单。</li><li>最多只能做到和示教数据一样好，无法超越。</li></ol><p>强化学习的特点：</p><ol><li>需要奖励函数。</li><li>需要足够的探索。</li><li>有可能存在的不能收敛问题。</li><li>可以做到超越人类的决策。</li></ol><p>因此我们可以把两者结合起来，既有人类的经验，又有自己的探索和学习。我们的做法是进行预训练和微调。AlphaGo正是运用了这种框架。同样星际争霸2的AlphaStar同样也是这种训练框架，得到了超越人类的水平。</p><img src="/2021/10/10/mo-fang-xue-xi-imitation-learning/3.png" class=""><p>但在运用pretrain和finetune这种框架时我们通常会面临一个问题，就是在预训练过后进行强化学习的时候，我们的策略一开始采集到的数据很可能是非常糟糕的，这会直接摧毁策略网络，导致效果越来越差，训练没法进行。因此我们需要在策略中将一开始的示教数据保留下来，我们可以<strong>把示教的数据直接放入reply buffer中</strong>，这样可以让策略网络随时进行学习。</p><hr><p>我们可以通过加入一个损失函数同时对loss进行优化：</p> <img src="/2021/10/10/mo-fang-xue-xi-imitation-learning/4.png" class=""><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><img src="/2021/10/10/mo-fang-xue-xi-imitation-learning/5.png" class=""><img src="/2021/10/10/mo-fang-xue-xi-imitation-learning/6.png" class=""><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><img src="/2021/10/10/mo-fang-xue-xi-imitation-learning/7.png" class="">]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习之SAC</title>
      <link href="2021/10/06/qiang-hua-xue-xi-zhi-sac/"/>
      <url>2021/10/06/qiang-hua-xue-xi-zhi-sac/</url>
      
        <content type="html"><![CDATA[<p> 参考视频：<a href="https://www.bilibili.com/video/BV1EK41157fD/?spm_id_from=333.788.recommend_more_video.-1">周博磊强化学习课程</a> </p><p>价值函数优化学习主线：Q-learning→DQN→DDPG→TD3→SAC</p><p>Q-Learning，DQN和DDPG请可以参考我之前的文章：<a href="https://tianjuewudi.gitee.io/2021/07/16/qiang-hua-xue-xi-shi-jian-jiao-xue/#toc-heading-20">强化学习实践教学</a></p><p>TD3可以参考我之前的博客：<a href="https://blog.csdn.net/tianjuewudi/article/details/120626544">强化学习之TD3(pytorch实现)</a></p><p>SAC可以参考博客：<a href="https://blog.csdn.net/qq_38587510/article/details/104970837">https://blog.csdn.net/qq_38587510/article/details/104970837</a></p><p>参考论文：</p><ol><li><a href="https://arxiv.org/abs/1801.01290">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>，2018年8月发表。</li><li><a href="https://link.zhihu.com/?target=https://arxiv.org/pdf/1812.05905.pdf">Soft Actor-Critic Algorithms and Applications</a>，2019年1月发表。</li></ol><p>​       SAC全称Soft Actor-Critic，它整合了entropy regularization的思想。论文有以上两篇，第一篇采用模型包括一个actor网络，两个状态价值V网络，两个动作价值Q网络，第二篇的模型包括一个actor网络，四个动作价值Q网络。</p><p>​       model-free深度强化学习算法面临两个主要挑战：<strong>高采样复杂度和脆弱的收敛性，因此严重依赖调参</strong>，这两个挑战限制了强化学习向现实应用的推广。SAC引入了<strong>最大熵（Maximum Entropy）强化学习</strong>，要求actor在同时最大化期望和策略分布的熵，也就是说，在保证任务成果的同时希望策略尽可能的随机。</p><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>这里说明一下信息熵的概念：当一件事情发生的概率越小，这件事情的信息量越大，对于一个分布而言，信息量的计算方式是：<br>$$<br>H(U) = E[-\log p_i] = - \sum_{i=1}^n p_i log p_i<br>$$<br>因此actor输出总概率相加为1的情况下，动作概率的分布越散，越不集中于一个action，这个熵的值越大。对于连续动作领域来说，就是随机噪声的采样值越偏离平均值，越出现在高斯分布边缘，这个值越大。</p><h3 id="Maximum-Entropy"><a href="#Maximum-Entropy" class="headerlink" title="Maximum Entropy"></a>Maximum Entropy</h3><p>​       在标准强化学习中需要最大化累积期望reward：<br>$$<br>\sum_{t} E_{（s_t,a_t）～ p_{\pi}}[\gamma^t r(s_t,a_t)]<br>$$<br>​       在最大熵强化学习中需要优化的目标是：<br>$$<br>argmax_{\pi}\sum_{t} E_{\tau ～ \pi}[\gamma^t (R(s_t,a_r,s_{t+1})+ \alpha H(\pi(.|s_t)))]<br>$$<br>​       最大熵强化学习在标准的最大reward强化目标上增加了一个最大熵项，提高了探索能力和鲁棒性。既降低了采样复杂度，又提高了收敛稳定性。可以学到更多near-optimal的行为，也就是在一些状态下，可能存在多个动作都是最优的，那么使选择它们的概率相同，可以提高学习的速度。</p><p>此时有：<br>$$<br> V^{\pi}(s) =  E_{\tau ～ \pi}[\sum_t \gamma^t (R(s_t,a_r,s_{t+1})+ \alpha H(\pi(.|s_t))) | s_0 = s]<br>$$</p><p>$$<br>Q^{\pi}(s，a) =  E_{s’ ～ P,a’～ \pi }[R(s,a,s’)+ \gamma (Q^{\pi}(s’,a’)  + \alpha H(\pi(.|s_t))) ] \<br>=  E_{s’ ～ P,a’～ \pi }[R(s,a,s’)+ \gamma (Q^{\pi}(s’,a’)  - \alpha \log \pi(a’|s’)) ]<br>$$</p><p>因此，采样更新公式应该是：<br>$$<br>Q^{\pi}(s，a) \approx   r + \gamma (Q^{\pi}(s’,\hat{ a’})  - \alpha \log \pi(\hat {a’}|s’)) ,\hat{a’} ～ \pi(.|s’)<br>$$<br>损失函数为：<br>$$<br>L（\phi_i,D） = E[(Q_{\phi}(s,a) - y(r,s’,d))^2]<br>$$<br>TD3类似，SAC也采用了两个Q网络的更新形式，然后使用较小的Q值进行更新。<br>$$<br>y(r,s’,d) =  r + \gamma (1-d)(\min_{j=1,2}Q_{\phi_{targ,j}}(s’,\hat{a’}) - \alpha \log \pi_{\theta}(\hat{a’}|s’)) ,\hat{a’} ～ \pi(.|s’)<br>$$<br>对于状态价值V有：<br>$$<br>V^{\pi}(s) = E_{a ～ \pi}[Q^{\pi}(s,a)] + \alpha H(\pi(.|s))  \<br> = E_{a ～ \pi}[Q^{\pi}(s,a)] - \alpha \log \pi(a|s))<br>$$</p><h3 id="Reparameterization-Trick"><a href="#Reparameterization-Trick" class="headerlink" title="Reparameterization Trick"></a>Reparameterization Trick</h3><p>同时也在动作输出中加入了噪声。这就是Reparameterization Trick。让a从策略网络中进行采样转变成和其没有什么关系的采样。<br>$$<br>\hat{a_{\theta}}(s,\epsilon) = \tanh(μ<em>{\theta}(s) + \sigma</em>{\theta}(s) \odot \epsilon),\epsilon～N(0,1)<br>$$<br>其中μ和σ都是由神经网络学习得到的，因此在奖励最大化的同时鼓励探索的话，σ会尽可能增大以达到探索的目的，这样一个μ和σ都会变化的动作分布大大增加了动作策略的灵活性。</p><h2 id="actor部分代码："><a href="#actor部分代码：" class="headerlink" title="actor部分代码："></a>actor部分代码：</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">GaussianPolicy</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_inputs<span class="token punctuation">,</span> num_actions<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> action_space<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>GaussianPolicy<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linear1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linear2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>mean_linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> num_actions<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>log_std_linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> num_actions<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 使用对应函数初始化所有的全连接层参数</span>        self<span class="token punctuation">.</span>apply<span class="token punctuation">(</span>weights_init_<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># action rescaling</span>        <span class="token keyword">if</span> action_space <span class="token keyword">is</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>action_scale <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>action_bias <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>action_scale <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>                <span class="token punctuation">(</span>action_space<span class="token punctuation">.</span>high <span class="token operator">-</span> action_space<span class="token punctuation">.</span>low<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>action_bias <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>                <span class="token punctuation">(</span>action_space<span class="token punctuation">.</span>high <span class="token operator">+</span> action_space<span class="token punctuation">.</span>low<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>linear1<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>linear2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        mean <span class="token operator">=</span> self<span class="token punctuation">.</span>mean_linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        log_std <span class="token operator">=</span> self<span class="token punctuation">.</span>log_std_linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        log_std <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>log_std<span class="token punctuation">,</span> min<span class="token operator">=</span>LOG_SIG_MIN<span class="token punctuation">,</span> max<span class="token operator">=</span>LOG_SIG_MAX<span class="token punctuation">)</span>        <span class="token keyword">return</span> mean<span class="token punctuation">,</span> log_std    <span class="token keyword">def</span> <span class="token function">sample</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>        mean<span class="token punctuation">,</span> log_std <span class="token operator">=</span> self<span class="token punctuation">.</span>forward<span class="token punctuation">(</span>state<span class="token punctuation">)</span>        std <span class="token operator">=</span> log_std<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 创建高斯分布</span>        normal <span class="token operator">=</span> Normal<span class="token punctuation">(</span>mean<span class="token punctuation">,</span> std<span class="token punctuation">)</span>        x_t <span class="token operator">=</span> normal<span class="token punctuation">.</span>rsample<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># for reparameterization trick (mean + std * N(0,1))</span>        y_t <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x_t<span class="token punctuation">)</span>        action <span class="token operator">=</span> y_t <span class="token operator">*</span> self<span class="token punctuation">.</span>action_scale <span class="token operator">+</span> self<span class="token punctuation">.</span>action_bias        log_prob <span class="token operator">=</span> normal<span class="token punctuation">.</span>log_prob<span class="token punctuation">(</span>x_t<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Enforcing Action Bound</span>        <span class="token comment" spellcheck="true"># ---重点问题---</span>        log_prob <span class="token operator">-=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>self<span class="token punctuation">.</span>action_scale <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> y_t<span class="token punctuation">.</span>pow<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">+</span> epsilon<span class="token punctuation">)</span>        log_prob <span class="token operator">=</span> log_prob<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>mean<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>action_scale <span class="token operator">+</span> self<span class="token punctuation">.</span>action_bias        <span class="token keyword">return</span> action<span class="token punctuation">,</span> log_prob<span class="token punctuation">,</span> mean    <span class="token keyword">def</span> <span class="token function">to</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>action_scale <span class="token operator">=</span> self<span class="token punctuation">.</span>action_scale<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>action_bias <span class="token operator">=</span> self<span class="token punctuation">.</span>action_bias<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        <span class="token keyword">return</span> super<span class="token punctuation">(</span>GaussianPolicy<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习之TD3</title>
      <link href="2021/10/06/qiang-hua-xue-xi-zhi-td3/"/>
      <url>2021/10/06/qiang-hua-xue-xi-zhi-td3/</url>
      
        <content type="html"><![CDATA[<p> 参考视频：<a href="https://www.bilibili.com/video/BV1EK41157fD/?spm_id_from=333.788.recommend_more_video.-1">https://www.bilibili.com/video/BV1EK41157fD/?spm_id_from=333.788.recommend_more_video.-1</a> </p><p>原论文：<a href="https://arxiv.org/abs/1802.09477">https://arxiv.org/abs/1802.09477</a></p><p>价值函数优化学习主线：Q-learning→DQN→DDPG→TD3→SAC</p><p>Q-Learning，DQN和DDPG请可以参考我之前的文章：<a href="https://tianjuewudi.gitee.io/2021/07/16/qiang-hua-xue-xi-shi-jian-jiao-xue/#toc-heading-20">强化学习实践教学</a></p><p>首先DDPG是对DQN的扩展，使得只能用于离散动作空间的DQN扩展到连续动作空间，在方法上同样运用了经验回放和Target网络，同时是一个Actor-Critic方法，因此存在四个网络。</p><p>TD3也叫做Twin Delayed DDPG，全称Twin Delayed Deep Deterministic Policy Gradient。是基于DDPG的改进。同样DDPG也存在着跟DQN相同的缺陷，就是由于采用的是max最大动作价值的方式进行更新，使得动作价值远比实际要高（overestinmate），使得训练不稳定。因此TD3提出了三种改进。</p><h2 id="Clipped-Double-Q-Learning"><a href="#Clipped-Double-Q-Learning" class="headerlink" title="Clipped Double_Q Learning"></a>Clipped Double_Q Learning</h2><p>TD3使用了两个Q网络进行学习，因此是“twin”。而且采用两个网络中动作价值Q较小的值进行更新。两个网络同时使用一个target网络，因此它们同时都会产生一个预测值，我们只用预测值中较小的来对网络进行更新。</p><h2 id="“Delayed”-Policy-Updates"><a href="#“Delayed”-Policy-Updates" class="headerlink" title="“Delayed” Policy Updates"></a>“Delayed” Policy Updates</h2><p>TD3更新策略网络（及其Target网络）的频率要比价值网络慢，论文中推荐更新两次价值网络才更新一次策略网络。这样可以让两者解耦，关联度降低，从而可以克服overestinmation。</p><h2 id="Target-Policy-Smoothing"><a href="#Target-Policy-Smoothing" class="headerlink" title="Target Policy Smoothing"></a>Target Policy Smoothing</h2><p>TD3在策略网络的target网络中引入了噪声，让其更加积极去探索Q价值网络的错误。<br>$$<br>a_{TD3}(s’) = clip(μ<em>{\theta,targ}(s’) + clip(\epsilon,-c,c) ,a</em>{low},a_{high}),\epsilon ～ N(0,\sigma)<br>$$</p><h2 id="Pytorch实现"><a href="#Pytorch实现" class="headerlink" title="Pytorch实现"></a>Pytorch实现</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> copy<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> Fdevice <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)</span><span class="token comment" spellcheck="true"># Paper: https://arxiv.org/abs/1802.09477</span><span class="token keyword">class</span> <span class="token class-name">Actor</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">,</span> max_action<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Actor<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>max_action <span class="token operator">=</span> max_action    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>        a <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>l1<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">)</span>        a <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>l2<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>max_action <span class="token operator">*</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>self<span class="token punctuation">.</span>l3<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">Critic</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Critic<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Q1 architecture</span>        self<span class="token punctuation">.</span>l1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim <span class="token operator">+</span> action_dim<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Q2 architecture</span>        self<span class="token punctuation">.</span>l4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>state_dim <span class="token operator">+</span> action_dim<span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l5 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l6 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">,</span> action<span class="token punctuation">)</span><span class="token punctuation">:</span>        sa <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>state<span class="token punctuation">,</span> action<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        q1 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>l1<span class="token punctuation">(</span>sa<span class="token punctuation">)</span><span class="token punctuation">)</span>        q1 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>l2<span class="token punctuation">(</span>q1<span class="token punctuation">)</span><span class="token punctuation">)</span>        q1 <span class="token operator">=</span> self<span class="token punctuation">.</span>l3<span class="token punctuation">(</span>q1<span class="token punctuation">)</span>        q2 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>l4<span class="token punctuation">(</span>sa<span class="token punctuation">)</span><span class="token punctuation">)</span>        q2 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>l5<span class="token punctuation">(</span>q2<span class="token punctuation">)</span><span class="token punctuation">)</span>        q2 <span class="token operator">=</span> self<span class="token punctuation">.</span>l6<span class="token punctuation">(</span>q2<span class="token punctuation">)</span>        <span class="token keyword">return</span> q1<span class="token punctuation">,</span> q2    <span class="token keyword">def</span> <span class="token function">Q1</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">,</span> action<span class="token punctuation">)</span><span class="token punctuation">:</span>        sa <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>state<span class="token punctuation">,</span> action<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        q1 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>l1<span class="token punctuation">(</span>sa<span class="token punctuation">)</span><span class="token punctuation">)</span>        q1 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>l2<span class="token punctuation">(</span>q1<span class="token punctuation">)</span><span class="token punctuation">)</span>        q1 <span class="token operator">=</span> self<span class="token punctuation">.</span>l3<span class="token punctuation">(</span>q1<span class="token punctuation">)</span>        <span class="token keyword">return</span> q1<span class="token keyword">class</span> <span class="token class-name">TD3</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>        self<span class="token punctuation">,</span>        state_dim<span class="token punctuation">,</span>        action_dim<span class="token punctuation">,</span>        max_action<span class="token punctuation">,</span>        discount<span class="token operator">=</span><span class="token number">0.99</span><span class="token punctuation">,</span>        tau<span class="token operator">=</span><span class="token number">0.005</span><span class="token punctuation">,</span>        policy_noise<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span>        noise_clip<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span>        policy_freq<span class="token operator">=</span><span class="token number">2</span>    <span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>actor <span class="token operator">=</span> Actor<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">,</span> max_action<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>actor_target <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>actor_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">3e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>critic <span class="token operator">=</span> Critic<span class="token punctuation">(</span>state_dim<span class="token punctuation">,</span> action_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>critic_target <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>critic_optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">3e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>max_action <span class="token operator">=</span> max_action        self<span class="token punctuation">.</span>discount <span class="token operator">=</span> discount        self<span class="token punctuation">.</span>tau <span class="token operator">=</span> tau        self<span class="token punctuation">.</span>policy_noise <span class="token operator">=</span> policy_noise        self<span class="token punctuation">.</span>noise_clip <span class="token operator">=</span> noise_clip        self<span class="token punctuation">.</span>policy_freq <span class="token operator">=</span> policy_freq        self<span class="token punctuation">.</span>total_it <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">def</span> <span class="token function">select_action</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state<span class="token punctuation">)</span><span class="token punctuation">:</span>        state <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>state<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> replay_buffer<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>total_it <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># Sample replay buffer </span>        state<span class="token punctuation">,</span> action<span class="token punctuation">,</span> next_state<span class="token punctuation">,</span> reward<span class="token punctuation">,</span> not_done <span class="token operator">=</span> replay_buffer<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Select action according to policy and add clipped noise</span>            noise <span class="token operator">=</span> <span class="token punctuation">(</span>                torch<span class="token punctuation">.</span>randn_like<span class="token punctuation">(</span>action<span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>policy_noise            <span class="token punctuation">)</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>noise_clip<span class="token punctuation">,</span> self<span class="token punctuation">.</span>noise_clip<span class="token punctuation">)</span>            next_action <span class="token operator">=</span> <span class="token punctuation">(</span>                self<span class="token punctuation">.</span>actor_target<span class="token punctuation">(</span>next_state<span class="token punctuation">)</span> <span class="token operator">+</span> noise            <span class="token punctuation">)</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token operator">-</span>self<span class="token punctuation">.</span>max_action<span class="token punctuation">,</span> self<span class="token punctuation">.</span>max_action<span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># Compute the target Q value</span>            target_Q1<span class="token punctuation">,</span> target_Q2 <span class="token operator">=</span> self<span class="token punctuation">.</span>critic_target<span class="token punctuation">(</span>next_state<span class="token punctuation">,</span> next_action<span class="token punctuation">)</span>            target_Q <span class="token operator">=</span> torch<span class="token punctuation">.</span>min<span class="token punctuation">(</span>target_Q1<span class="token punctuation">,</span> target_Q2<span class="token punctuation">)</span>            target_Q <span class="token operator">=</span> reward <span class="token operator">+</span> not_done <span class="token operator">*</span> self<span class="token punctuation">.</span>discount <span class="token operator">*</span> target_Q        <span class="token comment" spellcheck="true"># Get current Q estimates</span>        current_Q1<span class="token punctuation">,</span> current_Q2 <span class="token operator">=</span> self<span class="token punctuation">.</span>critic<span class="token punctuation">(</span>state<span class="token punctuation">,</span> action<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Compute critic loss</span>        critic_loss <span class="token operator">=</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>current_Q1<span class="token punctuation">,</span> target_Q<span class="token punctuation">)</span> <span class="token operator">+</span> F<span class="token punctuation">.</span>mse_loss<span class="token punctuation">(</span>current_Q2<span class="token punctuation">,</span> target_Q<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Optimize the critic</span>        self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        critic_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Delayed policy updates</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>total_it <span class="token operator">%</span> self<span class="token punctuation">.</span>policy_freq <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>            <span class="token comment" spellcheck="true"># Compute actor losse</span>            actor_loss <span class="token operator">=</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">.</span>Q1<span class="token punctuation">(</span>state<span class="token punctuation">,</span> self<span class="token punctuation">.</span>actor<span class="token punctuation">(</span>state<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># Optimize the actor </span>            self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>            actor_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>actor_optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>            <span class="token comment" spellcheck="true"># Update the frozen target models</span>            <span class="token keyword">for</span> param<span class="token punctuation">,</span> target_param <span class="token keyword">in</span> zip<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>critic_target<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                target_param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tau <span class="token operator">*</span> param<span class="token punctuation">.</span>data <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>tau<span class="token punctuation">)</span> <span class="token operator">*</span> target_param<span class="token punctuation">.</span>data<span class="token punctuation">)</span>            <span class="token keyword">for</span> param<span class="token punctuation">,</span> target_param <span class="token keyword">in</span> zip<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>actor_target<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                target_param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>copy_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tau <span class="token operator">*</span> param<span class="token punctuation">.</span>data <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> self<span class="token punctuation">.</span>tau<span class="token punctuation">)</span> <span class="token operator">*</span> target_param<span class="token punctuation">.</span>data<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">save</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> filename<span class="token punctuation">)</span><span class="token punctuation">:</span>        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> filename <span class="token operator">+</span> <span class="token string">"_critic"</span><span class="token punctuation">)</span>        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>self<span class="token punctuation">.</span>critic_optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> filename <span class="token operator">+</span> <span class="token string">"_critic_optimizer"</span><span class="token punctuation">)</span>        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>self<span class="token punctuation">.</span>actor<span class="token punctuation">.</span>state_dict<span class="token